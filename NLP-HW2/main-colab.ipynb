{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -U --quiet pip wheel jupyter\n",
    "!pip install --quiet numpy pytorch-lightning rank-bm25 torch tqdm transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from typing import Any\n",
    "\n",
    "import numpy\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning as torch_lightning\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from rank_bm25 import BM25Okapi as BM25\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer as Tokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encoding_set):\n",
    "        self.dataset = encoding_set\n",
    "\n",
    "    def __getitem__(self, idx) -> (dict, dict):\n",
    "        labels: list = [\"q_start\", \"q_end\", \"start\", \"end\"]\n",
    "        data_batch: dict = {key: val[idx].clone().detach() for key, val in self.dataset.items() if key not in labels}\n",
    "        label_batch: dict = {key: torch.from_numpy(val[idx]) for key, val in self.dataset.items() if key in labels}\n",
    "        return data_batch, label_batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset.input_ids)\n",
    "\n",
    "\n",
    "class DataPreprocess:\n",
    "    def __init__(self, source_path: str, source_type: str = \"train\"):\n",
    "        self.source_path: str = source_path\n",
    "        self.source_type: str = source_type\n",
    "        self.source_size: int = 0\n",
    "        self.raw_data: dict = {\"question\": [], \"answer\": [], \"reference\": []}\n",
    "        self.raw_label: dict = {\"q_start\": [], \"q_end\": [], \"start\": [], \"end\": []}\n",
    "        self.__tokenizer_prams: dict = {\n",
    "            \"truncation\": True,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"return_offsets_mapping\": True,\n",
    "            \"return_overflowing_tokens\": True,\n",
    "            \"return_tensors\": \"pt\",\n",
    "        }\n",
    "\n",
    "        self.__get_raw_data()\n",
    "\n",
    "    def __get_raw_data(self) -> None:\n",
    "        if not path.exists(self.source_path):\n",
    "            print(\"Request source does not exists.\")\n",
    "            exit()\n",
    "        with open(self.source_path, \"r\", encoding=\"UTF-8\") as source:\n",
    "            for line in tqdm(source.readlines()):\n",
    "                reference, question, answer = map(str, line.strip(\"\\n\").split(\" ||| \"))\n",
    "                reference = [str(r).strip() for r in reference.strip().strip(\"<s>\").strip(\"</s>\").split(\"</s>  <s>\")]\n",
    "                bm25 = BM25([r.split(\" \") for r in reference])\n",
    "                mask = bm25.get_scores(question.split(\" \")) > 0\n",
    "                reference = [reference[i] for i in range(len(mask)) if mask[i]]\n",
    "                for r in reference:\n",
    "                    self.source_size += 1\n",
    "                    self.raw_data[\"question\"].append(question)\n",
    "                    self.raw_data[\"answer\"].append(answer)\n",
    "                    self.raw_data[\"reference\"].append(r)\n",
    "                    self.raw_label[\"q_start\"].append(0)\n",
    "                    self.raw_label[\"q_end\"].append(len(question))\n",
    "                    answer_start = r.find(answer)\n",
    "                    if answer_start == -1:\n",
    "                        self.raw_label[\"start\"].append(0)\n",
    "                        self.raw_label[\"end\"].append(0)\n",
    "                    else:\n",
    "                        self.raw_label[\"start\"].append(answer_start)\n",
    "                        self.raw_label[\"end\"].append(answer_start + len(answer))\n",
    "\n",
    "        print(\"Raw data got.\")\n",
    "\n",
    "    def get_dataset(self, tokenizer: Any) -> QADataset:\n",
    "        encoding_set = tokenizer(\n",
    "            self.raw_data[\"question\"],\n",
    "            self.raw_data[\"reference\"],\n",
    "            truncation=self.__tokenizer_prams[\"truncation\"],\n",
    "            padding=self.__tokenizer_prams[\"padding\"],\n",
    "            return_overflowing_tokens=self.__tokenizer_prams[\"return_overflowing_tokens\"],\n",
    "            return_offsets_mapping=self.__tokenizer_prams[\"return_offsets_mapping\"],\n",
    "            return_tensors=self.__tokenizer_prams[\"return_tensors\"],\n",
    "        )\n",
    "        labels: dict = {\"q_start\": [], \"q_end\": [], \"start\": [], \"end\": []}\n",
    "        for i in trange(self.source_size):\n",
    "            q_start = encoding_set.char_to_token(i, self.raw_label[\"q_start\"][i] + 1, 0)\n",
    "            q_end = encoding_set.char_to_token(i, self.raw_label[\"q_end\"][i] - 1, 0)\n",
    "            labels[\"q_start\"].append(numpy.array(q_start))\n",
    "            labels[\"q_end\"].append(numpy.array(q_end))\n",
    "            if self.raw_label[\"start\"][i] == 0 and self.raw_label[\"end\"][i] == 0:\n",
    "                labels[\"start\"].append(numpy.array(0))\n",
    "                labels[\"end\"].append(numpy.array(0))\n",
    "            else:\n",
    "                labels[\"start\"].append(numpy.array(encoding_set.char_to_token(i, self.raw_label[\"start\"][i], 1)))\n",
    "                labels[\"end\"].append(numpy.array(encoding_set.char_to_token(i, self.raw_label[\"end\"][i] - 1, 1)))\n",
    "        encoding_set.update(labels)\n",
    "        return QADataset(encoding_set)\n",
    "\n",
    "\n",
    "class QADataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_dir: str = \"source/\",\n",
    "        tokenizer_name: str = \"bert-base-uncased\",\n",
    "        num_worker: int = 4,\n",
    "        batch_size: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.source_dir: str = source_dir\n",
    "        self.__tokenizer = Tokenizer.from_pretrained(tokenizer_name, do_lower_case=True)\n",
    "        self.__num_worker: int = num_worker\n",
    "        self.__batch_size: int = batch_size\n",
    "        self.__train_set: QADataset = DataPreprocess(self.source_dir + \"train.txt\", \"train\").get_dataset(\n",
    "            self.__tokenizer\n",
    "        )\n",
    "        self.__val_set: QADataset = DataPreprocess(self.source_dir + \"val.txt\", \"val\").get_dataset(self.__tokenizer)\n",
    "        # self.__test_set: Any = None\n",
    "        self.__predict_set: QADataset = DataPreprocess(self.source_dir + \"test.txt\", \"predict\").get_dataset(\n",
    "            self.__tokenizer\n",
    "        )\n",
    "\n",
    "    # def setup(self, stage: str) -> None:\n",
    "    # \tval_set_split = int(len(self.__val_set) * 0.8)\n",
    "    # \ttest_set_split = len(self.__val_set) - val_set_split\n",
    "    # \tself.__val_set, self.__test_set = random_split(self.__val_set, [val_set_split, test_set_split])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.__train_set, shuffle=True, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.__val_set, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "    # def test_dataloader(self) -> DataLoader:\n",
    "    # \treturn DataLoader(self.__test_set, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.__predict_set, batch_size=self.__batch_size, num_workers=self.__num_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QAModel(torch_lightning.LightningModule):\n",
    "    def __init__(self, tokenizer: Any, learning_rate: float = 1e-4):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.__num_label: int = 4\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer: Any = tokenizer\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, self.__num_label)\n",
    "\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_batch[\"input_ids\"],\n",
    "            attention_mask=input_batch[\"attention_mask\"],\n",
    "            token_type_ids=input_batch[\"token_type_ids\"],\n",
    "            return_dict=True,\n",
    "        )\n",
    "        output = self.fc(bert_output[\"last_hidden_state\"])\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_predictions(output_batch: Tensor) -> (Tensor, Tensor, Tensor, Tensor):\n",
    "        q_start_batch, q_end_batch, start_batch, end_batch = torch.split(output_batch, 1, 2)\n",
    "        q_start_batch = q_start_batch.squeeze(-1).contiguous()\n",
    "        q_end_batch = q_end_batch.squeeze(-1).contiguous()\n",
    "        start_batch = start_batch.squeeze(-1).contiguous()\n",
    "        end_batch = end_batch.squeeze(-1).contiguous()\n",
    "\n",
    "        return q_start_batch, q_end_batch, start_batch, end_batch\n",
    "\n",
    "    def calculate_loss(self, output_batch: Tensor, label_batch: Tensor) -> Any:\n",
    "        q_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "\n",
    "        q_start_loss = self.loss_fn(q_start_batch, label_batch[\"q_start\"])\n",
    "        q_end_loss = self.loss_fn(q_end_batch, label_batch[\"q_end\"])\n",
    "        start_loss = self.loss_fn(start_batch, label_batch[\"start\"])\n",
    "        end_loss = self.loss_fn(end_batch, label_batch[\"end\"])\n",
    "\n",
    "        return q_start_loss + q_end_loss + start_loss + end_loss\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        print(\"Epoch: {} starting to train...\".format(self.current_epoch))\n",
    "\n",
    "    def training_step(self, input_batch, input_batch_idx) -> dict:\n",
    "        data_batch, label_batch = input_batch\n",
    "        output_batch = self.forward(data_batch)\n",
    "\n",
    "        loss = self.calculate_loss(output_batch, label_batch)\n",
    "\n",
    "        self.log(\"loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        print(\"training ends\")\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        print(\"Epoch: {} starting to validate...\".format(self.current_epoch))\n",
    "\n",
    "    def validation_step(self, input_batch, input_batch_idx) -> dict:\n",
    "        data_batch, label_batch = input_batch\n",
    "        output_batch = self.forward(data_batch)\n",
    "\n",
    "        loss = self.calculate_loss(output_batch, label_batch)\n",
    "\n",
    "        self.log(\"loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        print(\"validation ends\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predictions(predictions_batch: Tensor) -> list:\n",
    "        _, predictions_batch = predictions_batch.topk(1, dim=1)\n",
    "\n",
    "        return predictions_batch.squeeze(-1)\n",
    "\n",
    "    # def on_test_epoch_start(self) -> None:\n",
    "    # \tprint('start to test...')\n",
    "    #\n",
    "    # def test_step(self, input_batch, input_batch_idx) -> dict:\n",
    "    # \tdata_batch, label_batch = input_batch\n",
    "    # \toutput_batch = self.forward(data_batch)\n",
    "    #\n",
    "    # \tq_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "    # \tq_start_batch = self.get_predictions(q_start_batch)\n",
    "    # \tq_end_batch = self.get_predictions(q_end_batch)\n",
    "    # \tstart_batch = self.get_predictions(start_batch)\n",
    "    # \tend_batch = self.get_predictions(end_batch)\n",
    "    #\n",
    "    # \tpredictions = [[q_start_batch[i], q_end_batch[i], start_batch[i], end_batch[i]] for i in range(len(q_start_batch))]\n",
    "    # \ttruths = [\n",
    "    # \t\t[\n",
    "    # \t\t\tint(label_batch['q_start'][i]),\n",
    "    # \t\t\tint(label_batch['q_end'][i]),\n",
    "    # \t\t\tint(label_batch['start'][i]),\n",
    "    # \t\t\tint(label_batch['end'][i])\n",
    "    # \t\t]\n",
    "    # \t\tfor i in range(len(label_batch['q_start']))\n",
    "    # \t]\n",
    "    #\n",
    "    # \treturn {'predict': predictions, 'truth': truths}\n",
    "    #\n",
    "    # def test_epoch_end(self, batch_output) -> None:\n",
    "    # \tpredict = torch.Tensor([prediction for batch in batch_output for prediction in batch['predict']])\n",
    "    # \ttruth = torch.Tensor([prediction for batch in batch_output for prediction in batch['truth']])\n",
    "    #\n",
    "    # \tprint(predict)\n",
    "    # \tprint(truth)\n",
    "    #\n",
    "    # \tf1_score = MultilabelF1Score(num_labels=4, average='macro')\n",
    "    # \taccuracy = MultilabelAccuracy(num_labels=4, average='macro')\n",
    "    #\n",
    "    # \tprint(f1_score(predict, truth))\n",
    "    # \tprint('F1-Score: {:.2f}, Accuracy: {:.2f}'.format(f1_score(predict, truth), accuracy(predict, truth)))\n",
    "    #\n",
    "    # def on_test_epoch_end(self) -> None:\n",
    "    # \tprint('testing ends')\n",
    "\n",
    "    def on_predict_start(self) -> None:\n",
    "        print(\"start to predict...\")\n",
    "\n",
    "    def predict_step(self, input_batch, input_batch_idx, dataloader_idx: int = 0) -> dict:\n",
    "        data_batch, question_batch = input_batch\n",
    "        output_batch = self.forward(data_batch)\n",
    "\n",
    "        q_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "        q_start_batch = self.get_predictions(q_start_batch)\n",
    "        q_end_batch = self.get_predictions(q_end_batch)\n",
    "        start_batch = self.get_predictions(start_batch)\n",
    "        end_batch = self.get_predictions(end_batch)\n",
    "\n",
    "        questions = [\n",
    "            self.tokenizer.decode(encoding[q_start_batch[i] : q_end_batch[i] + 1])\n",
    "            for i, encoding in enumerate(data_batch[\"input_ids\"])\n",
    "        ]\n",
    "        answers = [\n",
    "            self.tokenizer.decode(encoding[start_batch[i] : end_batch[i] + 1])\n",
    "            for i, encoding in enumerate(data_batch[\"input_ids\"])\n",
    "        ]\n",
    "\n",
    "        return {\"questions\": questions, \"answers\": answers}\n",
    "\n",
    "    def on_predict_end(self) -> None:\n",
    "        print(\"prediction ends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epoch: int = 1\n",
    "source_dir: str = \"/content/drive/MyDrive/Colab Notebooks/NLP-HW2/source/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = QADataModule(source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instantiate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "model = QAModel(tokenizer)\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epoch,\n",
    "    accelerator=\"auto\",\n",
    "    default_root_dir=\"model\",\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    limit_train_batches=1,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    limit_predict_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "trainer.fit(model, datamodule=data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.freeze()\n",
    "# trainer.test(model, datamodule=data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "predictions = trainer.predict(model, datamodule=data_loader, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = [question for batch in predictions for question in batch[\"questions\"]]\n",
    "answers = [answer for batch in predictions for answer in batch[\"answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_path: str = source_dir + \"test-submit-out.txt\"\n",
    "\n",
    "with open(target_path, \"w\", encoding=\"UTF-8\") as target:\n",
    "    for question, answer in tqdm(zip(questions, answers)):\n",
    "        target.write(\"{} ||| {}\\n\".format(question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat 'source/test-submit-out.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
