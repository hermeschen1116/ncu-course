{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mount drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install necessary packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -U --quiet pip wheel jupyter\n",
    "!pip install --quiet numpy pytorch-lightning rank-bm25 torch tqdm transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import necessary packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from typing import Any\n",
    "\n",
    "import numpy\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning as torch_lightning\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from rank_bm25 import BM25Okapi as BM25\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer as Tokenizer\n",
    "from transformers import BertModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, encoding_set):\n",
    "\t\tself.dataset = encoding_set\n",
    "\n",
    "\tdef __getitem__(self, idx) -> (dict, dict):\n",
    "\t\tlabels: list = ['q_start', 'q_end', 'start', 'end']\n",
    "\t\tdata_batch: dict = {key: val[idx].clone().detach() for key, val in self.dataset.items() if key not in labels}\n",
    "\t\tlabel_batch: dict = {key: torch.from_numpy(val[idx]) for key, val in self.dataset.items() if key in labels}\n",
    "\t\treturn data_batch, label_batch\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset.input_ids)\n",
    "\n",
    "\n",
    "class DataPreprocess:\n",
    "\tdef __init__(self, source_path: str, source_type: str = 'train'):\n",
    "\t\tself.source_path: str = source_path\n",
    "\t\tself.source_type: str = source_type\n",
    "\t\tself.source_size: int = 0\n",
    "\t\tself.raw_data: dict = {'question': [], 'answer': [], 'reference': []}\n",
    "\t\tself.raw_label: dict = {'q_start': [], 'q_end': [], 'start': [], 'end': []}\n",
    "\t\tself.__tokenizer_prams: dict = {\n",
    "\t\t\t'truncation': True,\n",
    "\t\t\t'padding': 'max_length',\n",
    "\t\t\t'return_offsets_mapping': True,\n",
    "\t\t\t'return_overflowing_tokens': True,\n",
    "\t\t\t'return_tensors': 'pt'\n",
    "\t\t}\n",
    "\n",
    "\t\tself.__get_raw_data()\n",
    "\n",
    "\tdef __get_raw_data(self) -> None:\n",
    "\t\tif not path.exists(self.source_path):\n",
    "\t\t\tprint('Request source does not exists.')\n",
    "\t\t\texit()\n",
    "\t\twith open(self.source_path, 'r', encoding='UTF-8') as source:\n",
    "\t\t\tfor line in tqdm(source.readlines()):\n",
    "\t\t\t\treference, question, answer = map(str, line.strip('\\n').split(' ||| '))\n",
    "\t\t\t\treference = [str(r).strip() for r in reference.strip().strip('<s>').strip('</s>').split('</s>  <s>')]\n",
    "\t\t\t\tbm25 = BM25([r.split(' ') for r in reference])\n",
    "\t\t\t\tmask = bm25.get_scores(question.split(' ')) > 0\n",
    "\t\t\t\treference = [reference[i] for i in range(len(mask)) if mask[i]]\n",
    "\t\t\t\tfor r in reference:\n",
    "\t\t\t\t\tself.source_size += 1\n",
    "\t\t\t\t\tself.raw_data['question'].append(question)\n",
    "\t\t\t\t\tself.raw_data['answer'].append(answer)\n",
    "\t\t\t\t\tself.raw_data['reference'].append(r)\n",
    "\t\t\t\t\tself.raw_label['q_start'].append(0)\n",
    "\t\t\t\t\tself.raw_label['q_end'].append(len(question))\n",
    "\t\t\t\t\tanswer_start = r.find(answer)\n",
    "\t\t\t\t\tif answer_start == -1:\n",
    "\t\t\t\t\t\tself.raw_label['start'].append(0)\n",
    "\t\t\t\t\t\tself.raw_label['end'].append(0)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.raw_label['start'].append(answer_start)\n",
    "\t\t\t\t\t\tself.raw_label['end'].append(answer_start + len(answer))\n",
    "\n",
    "\t\tprint(\"Raw data got.\")\n",
    "\n",
    "\tdef get_dataset(self, tokenizer: Any) -> QADataset:\n",
    "\t\tencoding_set = tokenizer(\n",
    "\t\t\tself.raw_data['question'],\n",
    "\t\t\tself.raw_data['reference'],\n",
    "\t\t\ttruncation=self.__tokenizer_prams['truncation'],\n",
    "\t\t\tpadding=self.__tokenizer_prams['padding'],\n",
    "\t\t\treturn_overflowing_tokens=self.__tokenizer_prams['return_overflowing_tokens'],\n",
    "\t\t\treturn_offsets_mapping=self.__tokenizer_prams['return_offsets_mapping'],\n",
    "\t\t\treturn_tensors=self.__tokenizer_prams['return_tensors']\n",
    "\t\t)\n",
    "\t\tlabels: dict = {'q_start': [], 'q_end': [], 'start': [], 'end': []}\n",
    "\t\tfor i in trange(self.source_size):\n",
    "\t\t\tq_start = encoding_set.char_to_token(i, self.raw_label['q_start'][i] + 1, 0)\n",
    "\t\t\tq_end = encoding_set.char_to_token(i, self.raw_label['q_end'][i] - 1, 0)\n",
    "\t\t\tlabels['q_start'].append(numpy.array(q_start))\n",
    "\t\t\tlabels['q_end'].append(numpy.array(q_end))\n",
    "\t\t\tif self.raw_label['start'][i] == 0 and self.raw_label['end'][i] == 0:\n",
    "\t\t\t\tlabels['start'].append(numpy.array(0))\n",
    "\t\t\t\tlabels['end'].append(numpy.array(0))\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels['start'].append(numpy.array(encoding_set.char_to_token(i, self.raw_label['start'][i], 1)))\n",
    "\t\t\t\tlabels['end'].append(numpy.array(encoding_set.char_to_token(i, self.raw_label['end'][i] - 1, 1)))\n",
    "\t\tencoding_set.update(labels)\n",
    "\t\treturn QADataset(encoding_set)\n",
    "\n",
    "\n",
    "class QADataModule(pl.LightningDataModule):\n",
    "\tdef __init__(self, source_dir: str = 'source/', tokenizer_name: str = 'bert-base-uncased', num_worker: int = 4, batch_size: int = 8):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.source_dir: str = source_dir\n",
    "\t\tself.__tokenizer = Tokenizer.from_pretrained(tokenizer_name, do_lower_case=True)\n",
    "\t\tself.__num_worker: int = num_worker\n",
    "\t\tself.__batch_size: int = batch_size\n",
    "\t\tself.__train_set: QADataset = DataPreprocess(self.source_dir + 'train.txt', 'train').get_dataset(self.__tokenizer)\n",
    "\t\tself.__val_set: QADataset = DataPreprocess(self.source_dir + 'val.txt', 'val').get_dataset(self.__tokenizer)\n",
    "\t\t# self.__test_set: Any = None\n",
    "\t\tself.__predict_set: QADataset = DataPreprocess(self.source_dir + 'test.txt', 'predict').get_dataset(self.__tokenizer)\n",
    "\n",
    "\t# def setup(self, stage: str) -> None:\n",
    "\t# \tval_set_split = int(len(self.__val_set) * 0.8)\n",
    "\t# \ttest_set_split = len(self.__val_set) - val_set_split\n",
    "\t# \tself.__val_set, self.__test_set = random_split(self.__val_set, [val_set_split, test_set_split])\n",
    "\n",
    "\tdef train_dataloader(self) -> DataLoader:\n",
    "\t\treturn DataLoader(self.__train_set, shuffle=True, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "\tdef val_dataloader(self) -> DataLoader:\n",
    "\t\treturn DataLoader(self.__val_set, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "\t# def test_dataloader(self) -> DataLoader:\n",
    "\t# \treturn DataLoader(self.__test_set, batch_size=self.__batch_size, num_workers=self.__num_worker)\n",
    "\n",
    "\tdef predict_dataloader(self) -> DataLoader:\n",
    "\t\treturn DataLoader(self.__predict_set, batch_size=self.__batch_size, num_workers=self.__num_worker)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QAModel(torch_lightning.LightningModule):\n",
    "\tdef __init__(self, tokenizer: Any, learning_rate: float = 1e-4):\n",
    "\t\tsuper(QAModel, self).__init__()\n",
    "\n",
    "\t\tself.__num_label: int = 4\n",
    "\t\tself.learning_rate: float = learning_rate\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.tokenizer: Any = tokenizer\n",
    "\n",
    "\t\tself.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\t\tself.fc = nn.Linear(768, self.__num_label)\n",
    "\n",
    "\t\tself.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\tdef forward(self, input_batch):\n",
    "\t\tbert_output = self.bert(\n",
    "\t\t\tinput_ids=input_batch['input_ids'],\n",
    "\t\t\tattention_mask=input_batch['attention_mask'],\n",
    "\t\t\ttoken_type_ids=input_batch['token_type_ids'],\n",
    "\t\t\treturn_dict=True\n",
    "\t\t)\n",
    "\t\toutput = self.fc(bert_output['last_hidden_state'])\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef extract_predictions(output_batch: Tensor) -> (Tensor, Tensor, Tensor, Tensor):\n",
    "\t\tq_start_batch, q_end_batch, start_batch, end_batch = torch.split(output_batch, 1, 2)\n",
    "\t\tq_start_batch = q_start_batch.squeeze(-1).contiguous()\n",
    "\t\tq_end_batch = q_end_batch.squeeze(-1).contiguous()\n",
    "\t\tstart_batch = start_batch.squeeze(-1).contiguous()\n",
    "\t\tend_batch = end_batch.squeeze(-1).contiguous()\n",
    "\n",
    "\t\treturn q_start_batch, q_end_batch, start_batch, end_batch\n",
    "\n",
    "\tdef calculate_loss(self, output_batch: Tensor, label_batch: Tensor) -> Any:\n",
    "\t\tq_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "\n",
    "\t\tq_start_loss = self.loss_fn(q_start_batch, label_batch['q_start'])\n",
    "\t\tq_end_loss = self.loss_fn(q_end_batch, label_batch['q_end'])\n",
    "\t\tstart_loss = self.loss_fn(start_batch, label_batch['start'])\n",
    "\t\tend_loss = self.loss_fn(end_batch, label_batch['end'])\n",
    "\n",
    "\t\treturn q_start_loss + q_end_loss + start_loss + end_loss\n",
    "\n",
    "\tdef on_train_epoch_start(self) -> None:\n",
    "\t\tprint('Epoch: {} starting to train...'.format(self.current_epoch))\n",
    "\n",
    "\tdef training_step(self, input_batch, input_batch_idx) -> dict:\n",
    "\t\tdata_batch, label_batch = input_batch\n",
    "\t\toutput_batch = self.forward(data_batch)\n",
    "\n",
    "\t\tloss = self.calculate_loss(output_batch, label_batch)\n",
    "\n",
    "\t\tself.log('loss', loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\treturn {'loss': loss}\n",
    "\n",
    "\tdef on_train_epoch_end(self) -> None:\n",
    "\t\tprint('training ends')\n",
    "\n",
    "\tdef on_validation_epoch_start(self) -> None:\n",
    "\t\tprint('Epoch: {} starting to validate...'.format(self.current_epoch))\n",
    "\n",
    "\tdef validation_step(self, input_batch, input_batch_idx) -> dict:\n",
    "\t\tdata_batch, label_batch = input_batch\n",
    "\t\toutput_batch = self.forward(data_batch)\n",
    "\n",
    "\t\tloss = self.calculate_loss(output_batch, label_batch)\n",
    "\n",
    "\t\tself.log('loss', loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\treturn {'val_loss': loss}\n",
    "\n",
    "\tdef on_validation_epoch_end(self) -> None:\n",
    "\t\tprint('validation ends')\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef get_predictions(predictions_batch: Tensor) -> list:\n",
    "\t\t_, predictions_batch = predictions_batch.topk(1, dim=1)\n",
    "\n",
    "\t\treturn predictions_batch.squeeze(-1)\n",
    "\n",
    "\t# def on_test_epoch_start(self) -> None:\n",
    "\t# \tprint('start to test...')\n",
    "\t#\n",
    "\t# def test_step(self, input_batch, input_batch_idx) -> dict:\n",
    "\t# \tdata_batch, label_batch = input_batch\n",
    "\t# \toutput_batch = self.forward(data_batch)\n",
    "\t#\n",
    "\t# \tq_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "\t# \tq_start_batch = self.get_predictions(q_start_batch)\n",
    "\t# \tq_end_batch = self.get_predictions(q_end_batch)\n",
    "\t# \tstart_batch = self.get_predictions(start_batch)\n",
    "\t# \tend_batch = self.get_predictions(end_batch)\n",
    "\t#\n",
    "\t# \tpredictions = [[q_start_batch[i], q_end_batch[i], start_batch[i], end_batch[i]] for i in range(len(q_start_batch))]\n",
    "\t# \ttruths = [\n",
    "\t# \t\t[\n",
    "\t# \t\t\tint(label_batch['q_start'][i]),\n",
    "\t# \t\t\tint(label_batch['q_end'][i]),\n",
    "\t# \t\t\tint(label_batch['start'][i]),\n",
    "\t# \t\t\tint(label_batch['end'][i])\n",
    "\t# \t\t]\n",
    "\t# \t\tfor i in range(len(label_batch['q_start']))\n",
    "\t# \t]\n",
    "\t#\n",
    "\t# \treturn {'predict': predictions, 'truth': truths}\n",
    "\t#\n",
    "\t# def test_epoch_end(self, batch_output) -> None:\n",
    "\t# \tpredict = torch.Tensor([prediction for batch in batch_output for prediction in batch['predict']])\n",
    "\t# \ttruth = torch.Tensor([prediction for batch in batch_output for prediction in batch['truth']])\n",
    "\t#\n",
    "\t# \tprint(predict)\n",
    "\t# \tprint(truth)\n",
    "\t#\n",
    "\t# \tf1_score = MultilabelF1Score(num_labels=4, average='macro')\n",
    "\t# \taccuracy = MultilabelAccuracy(num_labels=4, average='macro')\n",
    "\t#\n",
    "\t# \tprint(f1_score(predict, truth))\n",
    "\t# \tprint('F1-Score: {:.2f}, Accuracy: {:.2f}'.format(f1_score(predict, truth), accuracy(predict, truth)))\n",
    "\t#\n",
    "\t# def on_test_epoch_end(self) -> None:\n",
    "\t# \tprint('testing ends')\n",
    "\n",
    "\tdef on_predict_start(self) -> None:\n",
    "\t\tprint('start to predict...')\n",
    "\n",
    "\tdef predict_step(self, input_batch, input_batch_idx, dataloader_idx: int = 0) -> dict:\n",
    "\t\tdata_batch, question_batch = input_batch\n",
    "\t\toutput_batch = self.forward(data_batch)\n",
    "\n",
    "\t\tq_start_batch, q_end_batch, start_batch, end_batch = self.extract_predictions(output_batch)\n",
    "\t\tq_start_batch = self.get_predictions(q_start_batch)\n",
    "\t\tq_end_batch = self.get_predictions(q_end_batch)\n",
    "\t\tstart_batch = self.get_predictions(start_batch)\n",
    "\t\tend_batch = self.get_predictions(end_batch)\n",
    "\n",
    "\t\tquestions = [self.tokenizer.decode(encoding[q_start_batch[i]: q_end_batch[i] + 1]) for i, encoding in enumerate(data_batch['input_ids'])]\n",
    "\t\tanswers = [self.tokenizer.decode(encoding[start_batch[i]: end_batch[i] + 1]) for i, encoding in enumerate(data_batch['input_ids'])]\n",
    "\n",
    "\t\treturn {'questions': questions, 'answers': answers}\n",
    "\n",
    "\tdef on_predict_end(self) -> None:\n",
    "\t\tprint('prediction ends')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epoch: int = 1\n",
    "source_dir: str = '/content/drive/MyDrive/Colab Notebooks/NLP-HW2/source/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load source"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_loader = QADataModule(source_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instantiate model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model = QAModel(tokenizer)\n",
    "trainer = Trainer(\n",
    "\tmax_epochs=num_epoch,\n",
    "\taccelerator='auto',\n",
    "\tdefault_root_dir='model',\n",
    "\tlog_every_n_steps=1,\n",
    "\tcheck_val_every_n_epoch=1,\n",
    "\tlimit_train_batches=1,\n",
    "\tlimit_val_batches=1,\n",
    "\tlimit_test_batches=1,\n",
    "\tlimit_predict_batches=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.unfreeze()\n",
    "trainer.fit(model, datamodule=data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.freeze()\n",
    "# trainer.test(model, datamodule=data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "predictions = trainer.predict(model, datamodule=data_loader, return_predictions=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "questions = [question for batch in predictions for question in batch['questions']]\n",
    "answers = [answer for batch in predictions for answer in batch['answers']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_path: str = source_dir + 'test-submit-out.txt'\n",
    "\n",
    "with open(target_path, 'w', encoding='UTF-8') as target:\n",
    "\tfor question, answer in tqdm(zip(questions, answers)):\n",
    "\t\ttarget.write('{} ||| {}\\n'.format(question, answer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cat 'source/test-submit-out.txt'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
