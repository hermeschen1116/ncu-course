{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6oqGiIXvrMl"
   },
   "source": [
    "# CS224N: PyTorch Tutorial (Winter '21)  CS224N：Pytorch教程（2021年冬）\n",
    "\n",
    "### Author: Dilara Soylu\n",
    "### 翻譯&練習題：修改自DeepL自動翻譯結果\n",
    "\n",
    "In this notebook, we will have a basic introduction to `PyTorch` and work on a toy NLP task. Following resources have been used in preparation of this notebook:\n",
    "* [\"Word Window Classification\" tutorial notebook]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n",
    "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
    "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n",
    "\n",
    "Many thanks to Angelica Sun and John Hewitt for their feedback.\n",
    "\n",
    "\n",
    "在這本筆記本中，我們將對 \"PyTorch\"進行基本介紹，並實作一個簡單的範例NLP任務。在準備本notebook時，我們使用了以下資源：\n",
    "* [\"單詞窗口分類 \"教程筆記本]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb)，作者Matt Lamm，來自CS224N的2020年冬季課程。\n",
    "* PyTorch官方文檔[Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) 作者：Soumith Chintala\n",
    "* PyTorch教學筆記本，[構建基本生成對抗網路（GANs）|Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans)，作者Sharon Zhou，在Coursera提供。\n",
    "\n",
    "非常感謝 Angelica Sun 和 John Hewitt 的回饋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gk1UKaNvrMv"
   },
   "source": [
    "# Part I.\n",
    "## Introduction 簡介\n",
    "[PyTorch](https://pytorch.org/) is a machine learning framework that is used in both academia and industry for various applications. PyTorch started of as a more flexible alternative to [TensorFlow](https://www.tensorflow.org/), which is another popular machine learning framework. At the time of its release, `PyTorch` appealed to the users due to its user friendly nature: as opposed to defining static graphs before performing an operation as in `TensorFlow`, `PyTorch` allowed users to define their operations as they go, which is also the approached integrated by `TensorFlow` in its following releases. Although `TensorFlow` is more widely preferred in the industry, `PyTorch` is often times the preferred machine learning framework for researchers. If you would like to learn more about the differences between the two, you can check out [this](https://blog.udacity.com/2020/05/pytorch-vs-tensorflow-what-you-need-to-know.html) blog post. \n",
    "\n",
    "[PyTorch](https://pytorch.org/)是一個機器學習框架，在學術界和工業界都被用於各種應用。`PyTorch` 最初是作為[TensorFlow](https://www.tensorflow.org/)的一個更靈活的替代品，後者是另一個流行的機器學習框架。在發布時，`PyTorch` 因其用戶友好的特性而吸引了使用者：相對於像 `TensorFlow` 那樣在執行操作前定義靜態圖，`PyTorch` 允許用戶在執行時定義他們的操作，`TensorFlow` 在其後續版本中也采用了這樣的方法。盡管 `TensorFlow` 在業界中更受青睞，但 `PyTorch` 通常是研究人員首選的機器學習框架。如果你想了解更多關於兩者之間的區別，你可以查看[這篇](https://blog.udacity.com/2020/05/pytorch-vs-tensorflow-what-you-need-to-know.html)部落格文章。\n",
    "\n",
    "Now that we have learned enough about the background of `PyTorch`, let's start by importing it into our notebook. To install `PyTorch`, you can follow the instructions here. Alternatively, you can open this notebook using `Google Colab`, which already has `PyTorch` installed in its base kernel. Once you are done with the installation process, run the following cell:\n",
    "\n",
    "現在我們已經充分了解了 `PyTorch` 的背景，讓我們開始把它導入我們的筆記本。要安裝 `PyTorch`，你可以按照這裡的說明。或者，你也可以用 `Google Colab` 打開這個筆記本，它的基本內核已經安裝了 `PyTorch`。一旦你完成了安裝過程，執行以下Cell："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xFtTl5c7ZrTU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import pprint, module we use for making our print statements prettier\n",
    "# 導入pprint，用來使我們的print結果更漂亮\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k10ZRdcBwDP3"
   },
   "source": [
    "We are all set to start our tutorial. Let's dive in!\n",
    "\n",
    "設定完畢，以下開始教學"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLdSN9ZXvrM0"
   },
   "source": [
    "## Tensors 張量\n",
    "\n",
    "Tensors are the most basic building blocks in `PyTorch`.  Tensors are similar to matrices, but the have extra properties and they can represent higher dimensions. For example, an square image with 256 pixels in both sides can be represented by a `3x256x256` tensor, where the first 3 dimensions represent the color channels, red, green and blue. \n",
    "\n",
    "張量是 `PyTorch` 中最基本的構建模塊。 張量類似於矩陣，但它們有額外的屬性，可以表示更高的維度。例如，一個兩邊有256個像素的正方形圖像可以用 \"3x256x256 \"張量來表示，其中前3個維度代表顏色通道，即紅、綠和藍。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6aWvTEy7lgG"
   },
   "source": [
    "### Tensor Initialization 張量初始化\n",
    "\n",
    "There are several ways to instantiate tensors in `PyTorch`, which we will go through next. \n",
    "\n",
    "在 `PyTorch` 中，有幾種實例化張量的方法，我們接下來將介紹這些方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c78_3AMEyvJd"
   },
   "source": [
    "#### **From a Python List** 從 Python List 初始化張量\n",
    "\n",
    "We can initalize a tensor from a `Python` list, which could include sublists. The dimensions and the data types will be automatically inferred by `PyTorch` when we use [`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html). \n",
    "\n",
    "我們可以從一個 `Python` 列表中初始化一個張量，該列表可以包括子列表。當我們使用[`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html)時，尺寸和數據類型將由`PyTorch`自動推斷出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lsjIW9I_ztiO",
    "outputId": "08675955-630d-43d2-b64c-76a957de0e41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor from a Python List\n",
    "data = [[0, 1], [2, 3], [4, 5]]\n",
    "x_python = torch.tensor(data)\n",
    "\n",
    "# Print the tensor\n",
    "x_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv6ZEoZ0RWb5"
   },
   "source": [
    "We can also call `torch.tensor()` with the optional `dtype` parameter, which will set the data type. Some useful datatypes to be familiar with are: `torch.bool`, `torch.float`, and `torch.long`.\n",
    "\n",
    "我們也可以在使用 `torch.tensor()` 時選擇 `dtype` 參數，可以設置資料型態。一些需要熟悉的有用的資料型態有 `torch.bool`, `torch.float`, 和`torch.long`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bQF5IhsD7-n",
    "outputId": "eb7bb916-4890-4058-c907-24c5fdda0df4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using the dtype to create a tensor of particular type\n",
    "x_float = torch.tensor(data, dtype=torch.float)\n",
    "x_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16VoILaaE-_j",
    "outputId": "9dbc363e-a44f-48cb-b18c-a81ba53b5613"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using the dtype to create a tensor of particular type\n",
    "x_bool = torch.tensor(data, dtype=torch.bool)\n",
    "x_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4HccPWFEQUB"
   },
   "source": [
    "We can also get the same tensor in our specified data type using methods such as `float()`, `long()` etc. \n",
    "\n",
    "我們也可以使用 `float()`, `long()` 等方法獲得我們指定的資料型態的相同張量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nh_yq0SuTS_W",
    "outputId": "bb2f8862-e971-4c6d-c523-8531f2de01a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_python.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFiS1OFdTlKE"
   },
   "source": [
    "We can also use `tensor.FloatTensor`, `tensor.LongTensor`, `tensor.Tensor` classes to instantiate a tensor of particular type. `LongTensor`s are particularly important in NLP as many methods that deal with indices require the indices to be passed as a `LongTensor`, which is a 64 bit integer. \n",
    "\n",
    "我們也可以使用 `tensor.FloatTensor`, `tensor.LongTensor`, `tensor.Tensor` 類別來實例化一個特定類型的張量。`LongTensor`在NLP中特別重要，因為許多處理索引值的方法需要將索引值作為 `LongTensor` 傳遞，它是一個 64-bit 的整數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXXWZ1H2TkNN",
    "outputId": "76ae14e3-5438-41ec-dfdc-b668ea89c52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `torch.Tensor` defaults to float\n",
    "# Same as torch.FloatTensor(data)\n",
    "x = torch.Tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuLzDzsoytM2"
   },
   "source": [
    "#### **From a NumPy Array** 從Numpy Array初始化\n",
    "We can also initialize a tensor from a `NumPy` array. \n",
    "\n",
    "我們也可以從一個 `NumPy` 陣列中初始化一個張量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtSNe8X-2Pox",
    "outputId": "286f20ac-1573-4610-8702-000e6e8fc14b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize a tensor from a NumPy array\n",
    "ndarray = np.array(data)\n",
    "x_numpy = torch.from_numpy(ndarray)\n",
    "\n",
    "# Print the tensor\n",
    "x_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhtcBgum3OZ3"
   },
   "source": [
    "#### **From a Tensor** 從一個張量初始化\n",
    "We can also initialize a tensor from another tensor, using the following methods:\n",
    "\n",
    "* `torch.ones_like(old_tensor)`: Initializes a tensor of `1s`.\n",
    "* `torch.zeros_like(old_tensor)`: Initializes a tensor of `0s`.\n",
    "* `torch.rand_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a uniform distribution between `0` and `1`.\n",
    "* `torch.randn_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a normal distribution.\n",
    "\n",
    "All of these methods preserve the tensor properties of the original tensor passed in, such as the `shape` and `device`, which we will cover in a bit. \n",
    "\n",
    "我們也可以從另一個張量中初始化一個張量，使用以下方法。\n",
    "\n",
    "* `torch.ones_like(old_tensor)`: 初始化一個全部都是 `1` 的張量。\n",
    "* `torch.zeros_like(old_tensor)`: 初始化一個全部都是 `0` 的張量。\n",
    "* `torch.rand_like(old_tensor)`: 初始化一個張量，其中所有的元素都是從 `0` 和 `1` 之間的均勻分布中採樣的。\n",
    "* `torch.randn_like(old_tensor)`: 初始化一個張量，其中所有的元素都是從常態分布中採樣的。\n",
    "\n",
    "所有這些方法都保留了傳入的原始張量的張量屬性，比如 `shape`和 `device`，我們稍後會介紹這些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoKVhcLh2yqe",
    "outputId": "2542ad0a-6b71-40d0-d7da-2bc02d07fe82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a base tensor\n",
    "x = torch.tensor([[1.0, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FncfGN6z7ELA",
    "outputId": "729a5d2f-dee9-45bc-a80b-038d92e0243f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor of 0s\n",
    "x_zeros = torch.zeros_like(x)\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D993dpnP6iA8",
    "outputId": "f098cc8f-2583-4a0d-96b2-b9e94b751ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor of 1s\n",
    "x_ones = torch.ones_like(x)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBUDeEm97IqW",
    "outputId": "945dd0c1-4b54-4f87-80eb-9315cd24a2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8420, 0.0232],\n",
       "        [0.4553, 0.5807]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor where each element is sampled from a uniform distribution\n",
    "# between 0 and 1\n",
    "x_rand = torch.rand_like(x)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYsE3lKt7IEX",
    "outputId": "2cc16747-a9c7-4caf-deb6-e2a20a957857"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7074,  0.1293],\n",
       "        [-0.7968, -1.8529]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor where each element is sampled from a normal distribution\n",
    "x_randn = torch.randn_like(x)\n",
    "x_randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6tqf7v38vbi"
   },
   "source": [
    "#### **By Specifying a Shape** 透過指定一個大小來創建張量\n",
    "We can also instantiate tensors by specifying their shapes (which we will cover in more detail in a bit). The methods we could use follow the ones in the previous section:\n",
    "* `torch.zeros()`\n",
    "* `torch.ones()`\n",
    "* `torch.rand()`\n",
    "* `torch.randn()`\n",
    "\n",
    "我們也可以透過指定它們的大小來實例化張量（我們將在稍後詳細介紹）。我們可以使用的方法與上一節中的方法相同。\n",
    "* `torch.zeros()`\n",
    "* `torch.one()`\n",
    "* `torch.rand()`\n",
    "* `torch.randn()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dh4I4Npz-dZ4",
    "outputId": "0d800d12-9a1e-4d62-baaf-a5655eb61d49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a 4x3x2 tensor of 0s\n",
    "shape = (4, 3, 2)\n",
    "x_zeros = torch.zeros(shape)  # x_zeros = torch.zeros(4, 3, 2) is an alternative\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LEjeR24MLkN"
   },
   "source": [
    "#### **With `torch.arange()`** **使用 `torch.arange()`**\n",
    "\n",
    "We can also create a tensor with `torch.arange(end)`, which returns a `1-D` tensor with elements ranging from `0` to `end-1`. We can use the optional `start` and `step` parameters to create tensors with different ranges.  \n",
    "\n",
    "我們也可以用 `torch.arange(end)` 創建一個張量，它會回傳一個一維張量，元素範圍從 `0` 到 `end-1`。也可以使用 `start` 和 `step` 參數來創建不同範圍的張量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EjARl2aM7pA",
    "outputId": "ffbb0597-cbcf-4cd0-b3f7-f1f7dd0857a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with values 0-9\n",
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgpkRn527zSr"
   },
   "source": [
    "### Tensor Properties Tensor 屬性\n",
    "\n",
    "Tensors have a few properties that are important for us to cover. These are namely `shape`, and the `device` properties. \n",
    "\n",
    "Tensor的幾個屬性對我們來說很重要： 比如大小(`shape`)和設備(`device`)屬性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBt6e4xZT3zr"
   },
   "source": [
    "#### Data Type 資料類型\n",
    "\n",
    "The `dtype` property lets us see the data type of a tensor. \n",
    "\n",
    "`dtype` 屬性可以讓我們確認一個張量所儲存的資料型態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlF3k3eUT_hQ",
    "outputId": "70e01788-5d3d-437b-eaa1-dddc52d85266"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
    "x = torch.ones(3, 2)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1vtN4Dy8FAG"
   },
   "source": [
    "#### Shape 形狀\n",
    "\n",
    "The `shape` property tells us the shape of our tensor. This can help us identify how many dimensional our tensor is as well as how many elements exist in each dimension.\n",
    "\n",
    "`shape`屬性告訴我們 tensor 的大小。這可以幫助我們確定我們的 tensor 是多少維的，以及每個維度上有多少元素存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24gXLJcn7Pxs",
    "outputId": "fd601e85-8d77-4ff0-a783-641662291d0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
    "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vV0cE1cXAEHP",
    "outputId": "825530e9-4489-4114-e32f-5e6ff2ce8222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out its shape\n",
    "# Same as x.size()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MA3vnJnaAQlc",
    "outputId": "ef1032b2-b24e-4f3a-e05c-37e8c82d5226"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the number of elements in a particular dimension\n",
    "# 0th dimension corresponds to the rows\n",
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxXCX6y6BvhH"
   },
   "source": [
    "We can also get the size of a particular dimension with the `size()` method.\n",
    "\n",
    "我們也可以透過 `size()` 函式獲得某個特定維度的大小（相當於`shape[]`）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZapQmydxBVuy",
    "outputId": "86a58595-f855-401e-c53e-8f6a0f58358f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the size of the 0th dimension\n",
    "x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCQm7ToPOveH"
   },
   "source": [
    "We can change the shape of a tensor with the `view()` method.\n",
    "\n",
    "我們可以用 `view()` 函式改變張量的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1JH3fiNO5Gu",
    "outputId": "9358d006-c913-4542-8585-402da06e7e8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example use of view()\n",
    "# x_view shares the same memory as x, so changing one changes the other\n",
    "x_view = x.view(2, 3)\n",
    "x_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C3x4seqPGEI",
    "outputId": "64978794-8405-4b38-be16-80e228f4fabf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can ask PyTorch to infer the size of a dimension with -1\n",
    "x_view = x.view(3, -1)\n",
    "x_view.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYSCEesPITpf"
   },
   "source": [
    "We can also use `torch.reshape()` method for a similar purpose. There is a subtle difference between `reshape()` and `view()`: `view()` requires the data to be stored contiguously in the memory. You can refer to [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) StackOverflow answer for more information. In simple terms, contiguous means that the way our data is laid out in the memory is the same as the way we would read elements from it. This happens because some methods, such as `transpose()` and `view()`, do not actually change how our data is stored in the memory. They just change the meta information about out tensor, so that when we use it we will see the elements in the order we expect. \n",
    "\n",
    "我們也可以使用 `torch.reshape()` 函式來達到類似目的。`reshape()` 和 `view()` 之間有一個微妙的區別：`view()` 需要資料在記憶體中是被連續儲存的。你可以參考[這個](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)StackOverflow 的答案，了解更多資訊。簡單來說，連續意味著我們的資料在記憶體中的布局方式與我們從記憶體中讀取元素的方式是一樣的。這是因為一些函式，如 `transpose()` 和 `view()`，實際上並沒有改變我們的數據在內存中的存儲方式。它們只是改變了張量的元信息（meta information），所以當我們使用它時，我們會按照我們期望的順序看到元素。\n",
    "\n",
    "`reshape()` calls `view()` internally if the data is stored contiguously, if not, it returns a copy. The difference here isn't too important for basic tensors, but if you perform operations that make the underlying storage of the data non-contiguous (such as taking a transpose), you will have issues using `view()`. If you would like to match the way your tensor is stored in the memory to how it is used, you can use the `contiguous()` method.  \n",
    "\n",
    "如果資料是連續儲存的，`reshape()` 在內部呼叫 `view()`；如果不是，則返回一個副本。這裡的區別對於基本的張量來說並不太重要，但是如果你執行的操作使得資料的底層記憶體儲存不連續（比如進行矩陣轉置 `transpose()`），使用 `view()` 就會有問題。如果想讓你的張量在記憶體中的儲存方式與它的使用方式相匹配，你可以使用 `contiguous()` 函式。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLGcGYE4Llom",
    "outputId": "02a0c844-52f6-4beb-c828-65411d57e8cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the shape of x to be 2x3\n",
    "# x_reshaped could be a reference to or copy of x\n",
    "x_reshaped = torch.reshape(x, (2, 3))\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWNTZKZZQ9i6"
   },
   "source": [
    "We can use `torch.unsqueeze(x, dim)` function to add a dimension of size `1` to the provided `dim`, where `x` is the tensor. We can also use the corresponding use `torch.squeeze(x)`, which removes the dimensions of size `1`.\n",
    "\n",
    "我們可以使用 `torch.unsqueeze(x, dim)` 函數在提供的 `dim` 上增加一個大小為 `1` 的維度，其中 `x` 是張量。我們也可以使用相應的`torch.squeeze(x)`，刪除大小為 `1` 的維度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_IYojrJRh-m",
    "outputId": "27d63e66-b43b-4584-d240-1633fd6122f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a 5x2 tensor, with 5 rows and 2 columns\n",
    "x = torch.arange(10).reshape(5, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLhg_oZ4SHh-",
    "outputId": "83a9b289-675e-4c02-f6b5-f20d1bcc0268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new dimension of size 1 at the 1st dimension\n",
    "x = x.unsqueeze(1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoGYGbMRSo-J",
    "outputId": "9adc4bc2-87e1-4321-afde-507b85714a1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\n",
    "x = x.squeeze()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQpZ4556B3lb"
   },
   "source": [
    "If we want to get the total number of elements in a tensor, we can use the `numel()` method. \n",
    "\n",
    "如果我們想得到張量中元素的總數，我們可以使用 `numel()` 函式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-irUWlxTB6a",
    "outputId": "791e2f17-cbaf-4a5a-a7d8-1de15bc90af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76yVMMg_CA0Q",
    "outputId": "e5bee485-39b3-4591-b0f1-9d5f7467991a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of elements in tensor.\n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M1U_RTpBhl2"
   },
   "source": [
    "#### **Device** **設備**\n",
    "\n",
    "Device property tells `PyTorch` where to store our tensor. Where a tensor is stored determines which device, `GPU` or `CPU`, would be handling the computations involving it. We can find the device of a tensor with the `device` property.\n",
    "\n",
    "設備屬性告訴 `PyTorch` 將我們的張量儲存在哪裡。張量的儲存位置決定了哪個設備，即 `GPU` 或 `CPU`，這會關係到之後的計算流程。我們可以透過 `device` 屬性知道張量目前被儲存在哪個設備中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYRGhIbnCl3b",
    "outputId": "ceea3384-02fc-454f-d362-183f1ba7ba6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an example tensor\n",
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byEnJyKdBgjl",
    "outputId": "cb998753-a93d-4db9-8697-050500551727"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the device of the tensor\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vqf_-NADFX8"
   },
   "source": [
    "We can move a tensor from one device to another with the method `to(device)`.\n",
    "\n",
    "我們可以用 `to(device) `函式將一個張量從一個設備移動到另一個設備。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "GzA6zqkXDEt1"
   },
   "outputs": [],
   "source": [
    "# Check if a GPU is available, if so, move the tensor to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0A8j4q7Uu0f",
    "outputId": "cd4d1db6-5a05-4f95-e11a-d3d464d4f271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7BMktFFAkRA"
   },
   "source": [
    "### Tensor Indexing Tensor索引\n",
    "\n",
    "In `PyTorch` we can index tensors, similar to `NumPy`. \n",
    "\n",
    "在 `PyTorch` 中，我們可以對張量進行索引，類似於 `NumPy`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRJN7ovWDsKV",
    "outputId": "d2386431-c817-49fb-be26-268c76c6988f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an example tensor\n",
    "x = torch.Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M67ZiOF1Heyc",
    "outputId": "270645ed-107f-423b-8601-bf1853f23e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guXKE7m8AX1K",
    "outputId": "523f9fd9-4c71-4f23-cbb2-8fe4ffce9427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 0th element, which is the first row\n",
    "x[0]  # Equivalent to x[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8m8EyVvES4-"
   },
   "source": [
    "We can also index into multiple dimensions with `:`.\n",
    "\n",
    "我們也可以用 `:` 對多個維度進行索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Z6GFUcuEL85",
    "outputId": "5d1a4f0a-28da-4deb-c567-bcd647d20c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top left element of each element in our tensor\n",
    "x[:, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm8vc3nuXaEw"
   },
   "source": [
    "We can also access arbitrary elements in each dimension. \n",
    "\n",
    "我們也可以在每個維度上存取任意的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYhcH9gaWHyW",
    "outputId": "f83dc67d-1b22-44cf-c6ae-52de7bac49b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print x again to see our tensor\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4xl6CW3RrEw",
    "outputId": "bb7f2c21-c4bb-48e2-d435-1c2f41666803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's access the 0th and 1st elements, each twice\n",
    "i = torch.tensor([0, 0, 1, 1])\n",
    "x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3QYZ8k7Wvqp",
    "outputId": "e964d446-1a0f-46f4-9bbd-4959a293d4e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's access the 0th elements of the 1st and 2nd elements\n",
    "i = torch.tensor([1, 2])\n",
    "j = torch.tensor([0])\n",
    "x[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAELXC--IHS7"
   },
   "source": [
    "We can get a `Python` scalar value from a tensor with `item()`. \n",
    "\n",
    "我們可以用 `item()` 從張量中取得一個 `Python` 純量（scalar）值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM-ZujN2IGaQ",
    "outputId": "1acd40bb-d23a-4b12-fa6a-ad1a9a3a5c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NwxK7d_Ycgs",
    "outputId": "f4610fe8-0096-493a-e2d5-77eb5fa708da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GltnmDzeIXJM"
   },
   "source": [
    "### Operations 操作\n",
    "PyTorch operations are very similar to those of `NumPy`. We can work with both scalars and other tensors. \n",
    "\n",
    "`PyTorch` 的操作與 `NumPy` 的操作非常相似。我們可以用純量和一個張量進行運算，張量之間也能相互進行運算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9KBzcA0G6v9",
    "outputId": "b3b64730-c4f6-49ea-c529-b157e50dd2e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an example tensor\n",
    "x = torch.ones((3, 2, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUw8MAHqKuzs",
    "outputId": "71bb7a03-80fe-42ed-cad7-a6e4dcaf45c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[3., 3.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[3., 3.],\n",
       "         [3., 3.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform elementwise addition\n",
    "# Use - for subtraction\n",
    "x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAfMsaz1Gw5v",
    "outputId": "7812d5da-66ee-434d-d5e6-b3101c357ae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform elementwise multiplication\n",
    "# Use / for division\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aq89FU7OOe7"
   },
   "source": [
    "We can apply the same operations between different tensors of compatible sizes.\n",
    "\n",
    "我們可以在大小兼容的不同張量之間進行同樣的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGhz62wILIfN",
    "outputId": "6c90a11a-fe71-43ee-d5bd-27466d31ad0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 4x3 tensor of 6s\n",
    "a = torch.ones((4, 3)) * 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvDC1OzzPyLV",
    "outputId": "63a67f82-2726-4cdf-8d89-eb31d6c7789f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 1D tensor of 2s\n",
    "b = torch.ones(3) * 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUF9noMTP5NI",
    "outputId": "8411c30f-dfe9-4439-b99c-c8300d3a9cd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide a by b\n",
    "a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTiVVbukXRct"
   },
   "source": [
    "We can use `tensor.matmul(other_tensor)` for matrix multiplication and `tensor.T` for transpose. Matrix multiplication can also be performed with `@`.\n",
    "\n",
    "我們可以使用 `tensor.matmul(other_tensor)` 進行矩陣乘法，`tensor.T` 進行轉置。矩陣乘法也可以用 `@` 進行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPoC_WcbXCw5",
    "outputId": "39c38611-4aeb-4104-a568-0150a370440c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 36., 36., 36.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative to a.matmul(b)\n",
    "# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n",
    "# is inferred\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2WEoQjVYBQ8",
    "outputId": "c57ebe0d-a26d-4096-af9d-a38be0d6ecda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(a.shape)\n",
    "pp.pprint(a.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PibNpxbYYf2"
   },
   "source": [
    "We can take the mean and standard deviation along a certain dimension with the methods `mean(dim)` and `std(dim)`. That is, if we want to get the mean `3x2` matrix in a `4x3x2` matrix, we would set the `dim` to be 0. We can call these methods with no parameter to get the mean and standard deviation for the whole tensor. To use `mean` and `std` our tensor should be a floating point type. \n",
    "\n",
    "我們可以用 `mean(dim)` 和 `std(dim)` 函式來取得某一維度的平均和標準差。也就是說，如果我們想在一個 `4x3x2` 的矩陣中得到 `3x2` 的平均值，我們可以將 `dim` 設為 0。我們可以在沒有參數的情況下呼叫這些函式來得到整個張量的平均值和標準差。要使用 `mean` 和 `std`，我們的張量必須是浮點數類別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a987teCtYg7R",
    "outputId": "b0459af6-6364-482e-d01d-c542ad5a878c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Mean: 2.5'\n",
      "'Mean in the 0th dimension: tensor([2.5000, 2.5000])'\n",
      "'Mean in the 1st dimension: tensor([1., 2., 3., 4.])'\n"
     ]
    }
   ],
   "source": [
    "# Create an example tensor\n",
    "m = torch.tensor([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0], [4.0, 4.0]])\n",
    "\n",
    "pp.pprint(\"Mean: {}\".format(m.mean()))\n",
    "pp.pprint(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n",
    "pp.pprint(\"Mean in the 1st dimension: {}\".format(m.mean(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd77stQ5VQVT"
   },
   "source": [
    "We can concatenate tensors using `torch.cat`.\n",
    "\n",
    "我們可以用 `torch.cat` 來連接張量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "advfDCOPK9Gw",
    "outputId": "c57a7f9f-e21a-44ef-df4b-df9a221844e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: torch.Size([4, 3])\n",
      "Shape after concatenation in dimension 0: torch.Size([12, 3])\n",
      "Shape after concatenation in dimension 1: torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate in dimension 0 and 1\n",
    "a_cat0 = torch.cat([a, a, a], dim=0)\n",
    "a_cat1 = torch.cat([a, a, a], dim=1)\n",
    "\n",
    "print(\"Initial shape: {}\".format(a.shape))\n",
    "print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n",
    "print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BveswZMOjtff"
   },
   "source": [
    "Most of the operations in `PyTorch` are not in place. However, `PyTorch` offers the in place versions of operations available by adding an underscore (`_`) at the end of the method name. \n",
    "\n",
    "`PyTorch` 中的大多數操作都不是就地操作（指：改變原物件的值）。但是，`PyTorch` 在方法名稱的後面加上下劃線（`_`），就可以提供原地操作的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ebr7nn-DaU3B",
    "outputId": "c2313ae7-a09c-46cf-df99-78583cede330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print our tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mP8-VtoHaKAc",
    "outputId": "617a5030-1a01-495d-a519-6a45a3ecb451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12.],\n",
       "        [12., 12., 12.],\n",
       "        [12., 12., 12.],\n",
       "        [12., 12., 12.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add() is not in place\n",
    "a = a.add(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mY0ojINbaayp",
    "outputId": "1238e03c-bcf0-4a1a-bd0c-9baa84e43328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24., 24., 24.],\n",
       "        [24., 24., 24.],\n",
       "        [24., 24., 24.],\n",
       "        [24., 24., 24.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add_() is in place\n",
    "a.add_(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e96kVstA8Cqz"
   },
   "source": [
    "## Exercises: Tensors 練習：張量\n",
    "\n",
    "Please complete the following code as required.\n",
    "\n",
    "請按照說明的要求完成下列程式碼。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2PFIXUi89to"
   },
   "source": [
    "### 1. Import the pytorch package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "qTTT2gtr8OYO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5oAbqP59D1U"
   },
   "source": [
    "### 2. Create a tensor of size 10 from python list (dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVhPHSPh9Thg",
    "outputId": "f8a61931-d32b-4394-9ba7-96e379662a3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i for i in range(1, 11)]\n",
    "x_python = torch.tensor(data)\n",
    "x_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ-XiUPW-Cfg"
   },
   "source": [
    "### 3. Create a tensor of data type bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pW-MW3ts-dFJ",
    "outputId": "8e848d88-2954-4ef0-bc86-91499aea735a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(3, 2).bool()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSzMdwUr9USi"
   },
   "source": [
    "### 4. Create a 3x3 tensor with values ranging from 0 to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fg-r573A9Z78",
    "outputId": "3430f07b-e34a-4c5a-a15e-2e7898fac5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i for i in range(9)]\n",
    "x = torch.tensor(data).reshape(3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGf1LNpn9eJ1"
   },
   "source": [
    "### 5. Create a 3x3x3 tensor with random values and initialize a new tensor with 1s from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z09eBDyd9hT9",
    "outputId": "4e9e9212-26e8-4b60-d711-f3268734b4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = (3, 3, 3)\n",
    "x = torch.randn(x_shape)\n",
    "ones_x = torch.ones_like(x)\n",
    "ones_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1AFe6Fu-sH4"
   },
   "source": [
    "### 6. Output the data type, shape, and device of the following tensor and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP8JlP32_FFf",
    "outputId": "bb9b4f51-cd47-4287-8cd0-e025242e77bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: torch.int64\n",
      "Data Shape: torch.Size([4, 1, 3])\n",
      "Data Device: cpu\n"
     ]
    }
   ],
   "source": [
    "data = [[[3, 5, 6]], [[2, 1, 3]], [[9, 6, 0]], [[4, -1, 2]]]\n",
    "\n",
    "t = torch.tensor(data)\n",
    "\n",
    "print(\"Data Type: {}\".format(t.dtype))  # data type\n",
    "print(\"Data Shape: {}\".format(t.shape))  # shape\n",
    "print(\"Data Device: {}\".format(t.device))  # device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_o0btRcAMed"
   },
   "source": [
    "### 7. Get the top right element of each element in the tensor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ri4k7qyFAYGi",
    "outputId": "e0a9233b-d5b6-4ed0-af0a-a918e8bc8bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [3],\n",
       "        [0],\n",
       "        [2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Output will be:\n",
    "tensor([[6],\n",
    "        [3],\n",
    "        [0],\n",
    "        [2]])\n",
    "\"\"\"\n",
    "\n",
    "t[:, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WGOKh5yA5J_"
   },
   "source": [
    "### 8. Calculate the matrix multiplication of the following two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taFK96pgBDDf",
    "outputId": "92f85566-64f5-4f56-e5bf-ab106751b294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(12, 4, 8)\n",
    "t2 = torch.rand(12, 8, 16)\n",
    "\n",
    "t_mul = t1 @ t2\n",
    "t_mul.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re8xiL37eAja"
   },
   "source": [
    "# Part II.\n",
    "## Autograd 自動微分\n",
    "\n",
    "`PyTorch` and other machine learning libraries are known for their automatic differantiation feature. That is, given that we have defined the set of operations that need to be performed, the framework itself can figure out how to compute the gradients. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute.\n",
    "\n",
    "`PyTorch` 和其他機器學習函式庫以其自動微分的功能而聞名。也就是說，考慮到我們已經定義了需要執行的一系列操作，框架本身就可以想辦法計算梯度。我們可以調用 `backward()` 函式來要求 `PyTorch` 計算梯度，然後將其儲存在 `grad` 屬性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oEvBJHWfn8H",
    "outputId": "13487f48-b78a-4bdc-f701-8bb3ae89edf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create an example tensor\n",
    "# requires_grad parameter tells PyTorch to store gradients\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Print the gradient if it is calculated\n",
    "# Currently None since x is a scalar\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTJazZXkgthP",
    "outputId": "7445608b-4ebd-4e7a-d892-b94b38e12874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the gradient of y with respect to x\n",
    "y = x * x * 3  # 3x^2\n",
    "y.backward()\n",
    "pp.pprint(x.grad)  # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hqc2oM3iV6a"
   },
   "source": [
    "Let's run backprop from a different tensor again to see what happens.\n",
    "\n",
    "讓我們再對另一個不同的張量使用反向傳播，看看會發生什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K--Az0Xiic_z",
    "outputId": "b0844751-ffbe-4389-e01e-3e25efbf3cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3  # 3x^2\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhjPkiE6i7ja"
   },
   "source": [
    "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong. \n",
    "\n",
    "我們可以看到，`x.grad` 被更新為迄今為止計算的梯度之和。當我們在神經網路中運行反向傳播時，在進行更新之前，我們會將某個特定神經元的所有梯度相加，這正是這裡所發生的事情。這也是為什麼我們需要在每次訓練迭代中運行 `zero_grad()` 的原因（後面會詳細介紹）。否則，我們的梯度會在每個迭代中不斷累計，這將導致參數更新出錯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYLWqKIoaOyd"
   },
   "source": [
    "## Neural Network Module 神經網絡模組\n",
    "\n",
    "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it. \n",
    "\n",
    "到目前為止，我們已經研究了張量、它們的屬性和對張量的基本操作。如果我們要從頭開始構建網路層，熟悉這些內容是特別有用的。我們將在作業3(CS224N課程的)中利用這些東西，但在未來，我們將使用 `PyTorch `的 `torch.nn` 模組中的預定義模組。然後我們會把這些模組放在一起，創建複雜的網路。讓我們先用別名導入這個模組，這樣我們就不必每次使用時都要輸入 `torch`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qUmrDpbhV4Tn"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joGvRWjEbak0"
   },
   "source": [
    "### **Linear Layer** **線性層**\n",
    "\n",
    "We can use `nn.Linear(H_in, H_out)` to create a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`.\n",
    "\n",
    "我們可以使用 `nn.Linear(H_in, H_out)` 來創建一個線性層。這將接受一個 `(N, *, H_in)` 維度的矩陣並輸出一個 `(N, *, H_out)` 的矩陣。`*` 表示中間可以有任意多的維度。線性層執行 `Ax+b` 的操作，其中 `A` 和 `b` 是隨機初始化的。如果我們不希望線性層學習 `bias` 參數，我們可以用`bias=False` 來初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XfnKI4-a5j9",
    "outputId": "ff81ac8f-b6f4-458f-b592-9ad9e3de6612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709]],\n",
       "\n",
       "        [[-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the inputs\n",
    "input = torch.ones(2, 3, 4)\n",
    "# N* H_in -> N*H_out\n",
    "\n",
    "\n",
    "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
    "# dimensional outputs\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(input)\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_9XKtAFYpdI",
    "outputId": "5cd3ebda-d723-4c88-f939-69771b8b7db9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.4699, -0.1039, -0.4500,  0.1296],\n",
       "         [-0.4941,  0.3705, -0.4157,  0.3590]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3653,  0.1094], requires_grad=True)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear.parameters())  # Ax + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAXCCu9keUlW"
   },
   "source": [
    "### **Other Module Layers** **其他模塊層**\n",
    "\n",
    "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up. \n",
    "\n",
    "在 `nn` 模塊中還有其他幾個預設的層。一些常用的例子是 `nn.Conv2d`，`nn.ConvTranspose2d`，`nn.BatchNorm1d`，`nn.BatchNorm2d`，`nn.Upsample` 和 `nn.MaxPool2d` 以及其他等等。 隨著課程的進行，我們將進一步了解這些內容。目前唯一需要記住的是，我們可以把這些層當作即插即用的組件：我們只要提供所需的大小，`PyTorch` 會負責設置它們。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yslDOK66fYWn"
   },
   "source": [
    "### **Activation Function Layer** **激勵函數層**\n",
    "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in.\n",
    "\n",
    "我們還可以使用 `nn` 模組對我們的張量使用激勵函數。激勵函數被用來給我們的網路添加非線性。激勵函數的一些例子是 `nn.ReLU()`, `nn.Sigmoid()` 和 `nn.LeakyReLU()`。激勵函數是單獨對每個元素進行操作，所以我們得到的張量的大小和我們傳入的張量是一樣的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrJP5CveeOON",
    "outputId": "c47e3f69-ddb9-4ea2-a8e6-3ae7d30b5bf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709]],\n",
       "\n",
       "        [[-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709],\n",
       "         [-0.3197, -0.0709]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9v5FjQtd4Ck",
    "outputId": "6af95a5d-47ef-404b-f791-43f2eee12a72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4207, 0.4823],\n",
       "         [0.4207, 0.4823],\n",
       "         [0.4207, 0.4823]],\n",
       "\n",
       "        [[0.4207, 0.4823],\n",
       "         [0.4207, 0.4823],\n",
       "         [0.4207, 0.4823]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiYTthJwhEYT"
   },
   "source": [
    "### **Putting the Layers Together** **將多個層放在一起**\n",
    "\n",
    "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that. \n",
    "\n",
    "到目前為止我們已經學習到如何創建層，並將一個層的輸出作為下一個層的輸入。我們可以使用 `nn.Sequentual` 來代替創建中間張量並傳遞各層的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtJeOqLxhBLY",
    "outputId": "af00acbf-aa76-48f9-a6ea-0d1d24653030"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3866, 0.6464],\n",
       "         [0.3866, 0.6464],\n",
       "         [0.3866, 0.6464]],\n",
       "\n",
       "        [[0.3866, 0.6464],\n",
       "         [0.3866, 0.6464],\n",
       "         [0.3866, 0.6464]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(nn.Linear(4, 2), nn.Sigmoid())\n",
    "\n",
    "input = torch.ones(2, 3, 4)\n",
    "output = block(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkJ81p3GUVPM"
   },
   "source": [
    "### Custom Modules 自定義模組\n",
    "\n",
    "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n",
    "\n",
    "我們可以不使用預定義的模組，而是通過擴展 `nn.Module` 來建立自己的模組。例如，我們可以使用前面介紹的張量來建立一個 `nn.Linear`（它也擴展了`nn.Module`）。我們還可以建立新的、更複雜的模組，如自定義神經網路。你可以在CS224N後面的作業中練習這些。\n",
    "\n",
    "To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
    "\n",
    "要創建一個自定義模組，我們首先要做的是擴展 `nn.Module`。然後我們可以在 `__init__` 函數中初始化我們的參數，首先是呼叫父類別的 `__init__` 函數。我們定義的所有屬於 `nn` 模組對象的屬性都被當作參數，可以在訓練中學習。張量不是參數，但是如果它們被包在 `nn.Parameter` 類中，它們就可以變成參數。\n",
    "\n",
    "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`.\n",
    "\n",
    "所有擴展 `nn.Module` 的類別也應該實作一個 `forward(x)` 函數，其中 `x` 是一個張量。當一個參數被傳遞給我們的模組時，這是一個被呼叫的函數，例如在 `model(x)` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "J2P7eZiMj32_"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # Call to the __init__ function of the super class\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "        # Bookkeeping: Saving the initialization parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Defining of our model\n",
    "        # There isn't anything specific about the naming of `self.model`. It could\n",
    "        # be something arbitrary.\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2DrfLiBVjNT"
   },
   "source": [
    "Here is an alternative way to define the same class. You can see that we can replace `nn.Sequential` by defining the individual layers in the `__init__` method and connecting the in the `forward` method. \n",
    "\n",
    "下面是定義同一類別的另一種方法。你可以看到我們可以透過在 `__init__` 方法中定義各個層，並在 `forward` 函式中連接，來代替 `nn.Sequential`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "9-lqhsqwViIk"
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # Call to the __init__ function of the super class\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "        # Bookkeeping: Saving the initialization parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Defining of our layers\n",
    "        self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        relu = self.relu(linear)\n",
    "        linear2 = self.linear2(relu)\n",
    "        output = self.sigmoid(linear2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQelcFo5bXgU"
   },
   "source": [
    "Now that we have defined our class, we can instantiate it and see what it does. \n",
    "\n",
    "現在我們已經定義了我們的類別，我們可以把它實例化，看看它能作什麼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXi0T0FZbV0y",
    "outputId": "9bc6941c-7aa6-454c-fa25-4688bc76f2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3865, 0.4406, 0.5251, 0.4538, 0.5130],\n",
       "        [0.4015, 0.4533, 0.5372, 0.4204, 0.4530]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a sample input\n",
    "input = torch.randn(2, 5)\n",
    "\n",
    "# Create our model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Pass our input through our model\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCCbjc-Fb2-B"
   },
   "source": [
    "We can inspect the parameters of our model with `named_parameters()` and `parameters()` methods. \n",
    "\n",
    "我們可以用 `named_parameters()` 和 `parameters()` 函式檢查我們模型的參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d23soYIb2WZ",
    "outputId": "22096896-fbbc-4b48-dad6-8d347b92815d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear.weight', Parameter containing:\n",
       "  tensor([[ 0.0590, -0.2182, -0.1102,  0.1848,  0.1242],\n",
       "          [ 0.3129,  0.2491,  0.1808, -0.3214,  0.1112],\n",
       "          [-0.4008,  0.0207, -0.2799, -0.0014, -0.1322]], requires_grad=True)),\n",
       " ('linear.bias', Parameter containing:\n",
       "  tensor([0.2516, 0.3986, 0.2406], requires_grad=True)),\n",
       " ('linear2.weight', Parameter containing:\n",
       "  tensor([[ 0.1189,  0.5154, -0.2814],\n",
       "          [-0.0232, -0.1403, -0.3857],\n",
       "          [-0.0125,  0.1025, -0.1616],\n",
       "          [ 0.3783,  0.2448, -0.1698],\n",
       "          [ 0.3612,  0.0360,  0.3431]], requires_grad=True)),\n",
       " ('linear2.bias', Parameter containing:\n",
       "  tensor([-0.4914, -0.1494,  0.1398, -0.4161, -0.2637], requires_grad=True))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5JegycOdMFy"
   },
   "source": [
    "## Optimization 優化（器）\n",
    "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optimizers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well.\n",
    "\n",
    "我們已經展示了如何用 `backward()` 函數來計算梯度。有了梯度還不足以讓我們的模型進行學習。我們還需要知道如何更新我們模型的參數。這就是優化器登場的時候了。\n",
    "`torch.optim` 模組裡頭有幾個我們可以使用的優化器。一些比較常用的例子是 `optim.SGD` 和 `optim.Adam`。當初始化優化器時，我們傳遞我們的模型參數，可以用 `model.parameters()` 呼叫，告訴優化器它將優化哪些值。優化器也有一個學習率（`lr`）參數，它決定了每一步將進行多大的更新。不同的優化器也有不同的超參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "W0F-TvV0kk-I"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgak6o5dlQWF"
   },
   "source": [
    "After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data. \n",
    "\n",
    "有了優化函數後，我們可以定義一個我們想要優化的「損失」函數。我們可以自己定義損失，或者使用`PyTorch`中預定義的損失函數，如 `nn.BCELoss()`。現在讓我們把所有的東西放在一起！ 我們將從創建一些範例資料開始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGYFiaT_vXBn",
    "outputId": "233e6859-8fbe-404a-f589-f8758cd7af91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3453,  1.2104,  0.8543,  2.3149,  1.7949],\n",
       "        [ 0.1551,  3.4177,  0.5026,  0.9726,  0.3493],\n",
       "        [-0.0036,  0.1684,  0.9291,  1.8193,  0.4894],\n",
       "        [ 1.8198,  1.6284,  0.8045,  2.2575,  2.0745],\n",
       "        [ 1.6753, -0.5617,  0.4887,  1.7341,  2.1398],\n",
       "        [ 1.6839, -0.1710,  0.8256,  0.4285,  1.4306],\n",
       "        [-0.1116,  0.6421,  0.8681,  2.6186,  1.0652],\n",
       "        [ 1.8008,  1.3212,  0.1492,  0.3898, -0.7971],\n",
       "        [ 2.3707,  0.5358,  0.8470,  1.1773,  0.8518],\n",
       "        [ 1.7510, -0.9816,  1.6112,  0.4882, -0.2327]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the y data\n",
    "y = torch.ones(10, 5)\n",
    "\n",
    "# Add some noise to our goal y to generate our x\n",
    "# We want out model to predict our original data, albeit the noise\n",
    "x = y + torch.randn_like(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEsiOdpWvfLj"
   },
   "source": [
    "Now, we can define our model, optimizer and the loss function. \n",
    "\n",
    "現在，我們可以定義我們的模型、優化器和損失函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oA2XsdsbN8p",
    "outputId": "dcc9a14c-99c7-4fa7-fdc6-0a8a6d211d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066659092903137"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Define the optimizer\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "# Define loss using a predefined loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Calculate how our model is doing now\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtxU7Y8ZufSR"
   },
   "source": [
    "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop. \n",
    "\n",
    "讓我們看看是否能讓我們的模型達到一個較小的損失。現在我們有了我們需要的一切，我們可以設置我們的訓練循環。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogl6-Ctmuek6",
    "outputId": "1597ef88-8c38-46ed-d0b6-51368e05a5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 0.6066659092903137\n",
      "Epoch 1: training loss: 0.3982151746749878\n",
      "Epoch 2: training loss: 0.21716317534446716\n",
      "Epoch 3: training loss: 0.09739437699317932\n",
      "Epoch 4: training loss: 0.037612032145261765\n",
      "Epoch 5: training loss: 0.01358792744576931\n",
      "Epoch 6: training loss: 0.004923112224787474\n",
      "Epoch 7: training loss: 0.001856371178291738\n",
      "Epoch 8: training loss: 0.0007394045824185014\n",
      "Epoch 9: training loss: 0.00031236204085871577\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epoch, which determines the number of training iterations\n",
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Set the gradients to 0\n",
    "    adam.zero_grad()\n",
    "\n",
    "    # Get the model predictions\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Get the loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"Epoch {epoch}: training loss: {loss}\")\n",
    "\n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Take a step to optimize the weights\n",
    "    adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrMJ8AmqeCY-",
    "outputId": "58e250c4-42ed-4ce7-9637-aae7c730ecb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.8590, -0.2428, -0.5012, -0.3720, -0.0918],\n",
       "         [ 0.7235,  1.1215,  1.2097,  1.0908,  0.4797],\n",
       "         [ 1.2258,  0.4246,  0.7917,  0.7369,  0.7239]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4840,  0.8851,  1.2186], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0512,  0.7914,  1.0530],\n",
       "         [ 0.4277,  0.9813,  1.1057],\n",
       "         [ 0.9095,  1.2471,  1.2487],\n",
       "         [-0.0152,  1.3429,  0.2937],\n",
       "         [ 0.5647,  0.8968,  0.8212]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4949, 0.9367, 0.1443, 1.1922, 0.3211], requires_grad=True)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nXApd82wlsF"
   },
   "source": [
    "You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`. \n",
    "\n",
    "可以看到我們的損失正在減少。讓我們現在檢查一下我們模型的預測，看看它們是否接近我們最初的 `y`，也就是所有的 `1s`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRqE7P9EtvuS",
    "outputId": "7b5ca5c7-0e52-4f82-8820-ba991f42907e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9999, 0.9997, 0.9997, 0.9977, 0.9592],\n",
       "        [0.9998, 0.9992, 0.9992, 0.9952, 0.9422],\n",
       "        [0.9985, 0.9960, 0.9951, 0.9835, 0.8981],\n",
       "        [0.9999, 0.9996, 0.9996, 0.9970, 0.9542],\n",
       "        [1.0000, 0.9999, 1.0000, 0.9994, 0.9790],\n",
       "        [1.0000, 0.9998, 0.9998, 0.9982, 0.9634],\n",
       "        [0.9995, 0.9983, 0.9981, 0.9915, 0.9249],\n",
       "        [0.9972, 0.9935, 0.9917, 0.9762, 0.8799],\n",
       "        [0.9936, 0.9879, 0.9834, 0.9619, 0.8520],\n",
       "        [0.9984, 0.9957, 0.9948, 0.9827, 0.8958]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how our model performs on the training data\n",
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJng31_Pi2R6",
    "outputId": "b7fe361a-54a6-474e-b306-86636ec14bae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9986, 0.9996, 0.9998, 0.9983, 0.9974],\n",
       "        [0.9968, 0.9990, 0.9994, 0.9981, 0.9951],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9997, 0.9999, 1.0000, 0.9998, 0.9994],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9994, 0.9999, 0.9999, 0.9995, 0.9990],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test data and check how our model performs on it\n",
    "x2 = y + torch.randn_like(y)\n",
    "y_pred = model(x2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WNk6oIZw2xo"
   },
   "source": [
    "Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!\n",
    "\n",
    "很好！ 看起來我們的模型幾乎完美地學會了過濾掉我們傳入的 `x` 的噪音！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rUNk_1xG1v"
   },
   "source": [
    "# Part III.\n",
    "## Demo: Word Window Classification 演示：詞匯窗口分類\n",
    "\n",
    "Until this part of the notebook, we have learned the fundamentals of PyTorch and built a basic network solving a toy task. Now we will attempt to solve an example NLP task. Here are the things we will learn:\n",
    "\n",
    "在這部分筆記之前，我們已經學習了 `PyTorch` 的基礎知識，並建立了一個基本的網路來解決一個簡單的任務。現在我們將嘗試解決一個NLP任務的例子。以下是我們將要學習的內容：\n",
    "\n",
    "1. Data: Creating a Dataset of Batched Tensors\n",
    "2. Modeling\n",
    "3. Training\n",
    "4. Prediction\n",
    "---\n",
    "\n",
    "1. 數據：創建一個批次化的張量的數據集\n",
    "2. 建模\n",
    "3. 訓練\n",
    "4. 預測\n",
    "\n",
    "In this section, our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1` (meaning that `San Fransisco` won't be recognized as a `LOCATION`). Our task is called `Word Window Classification` for a reason. Instead of letting our model to only take a look at one word in each forward pass, we would like it to be able to consider the context of the word in question. That is, for each word, we want our model to be aware of the surrounding words. Let's dive in!\n",
    "\n",
    "在這一節中，我們的目標是訓練一個模型，在一個句子中找到與「地點」相對應的詞，這些詞的跨度總是 `1`（意味著 `San Fransisco` 不會被識別為 `地點`）。我們的任務被稱為「詞窗分類」是有原因的。我們不希望讓我們的模型在每次前進的過程中只看一個單詞，而是希望它能夠考慮到這個詞的文本脈絡。也就是說，對於每個詞，我們希望我們的模型能夠了解作為上下文的詞語。讓我們開始行動吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_amzuUx8BJXI"
   },
   "source": [
    "### Data 資料\n",
    "\n",
    "The very first task of any machine learning project is to set up our training set. Usually, there will be a training corpus we will be utilizing. In NLP tasks, the corpus would generally be a `.txt` or `.csv` file where each row corresponds to a sentence or a tabular datapoint. In our toy task, we will assume that we have already read our data and the corresponding labels into a `Python` list.\n",
    "\n",
    "機器學習的首要任務是建立我們的訓練集。通常情況下，我們會有一個訓練語料庫來使用。在NLP任務中，語料庫通常是一個 `.txt` 或 `.csv` 文件，每一行對應一個句子或一個表格資料點。在我們的簡易任務中，我們將假設我們已經將我們的數據和相應的標籤讀入一個 `Python` 的列表(list)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "mDiI1PLMw10z"
   },
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "    \"We always come to Paris\",\n",
    "    \"The professor is from Australia\",\n",
    "    \"I live in Stanford\",\n",
    "    \"He comes from Taiwan\",\n",
    "    \"The capital of Turkey is Ankara\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t33Uke9AE22s"
   },
   "source": [
    "#### Preprocessing 預處理\n",
    "\n",
    "To make it easier for our models to learn, we usually apply a few preprocessing steps to our data. This is especially important when dealing with text data. Here are some examples of text preprocessing:\n",
    "\n",
    "* **Tokenization**: Tokenizing the sentences into words.\n",
    "* **Lowercasing**: Changing all the letters to be lowercase.\n",
    "* **Noise removal:** Removing special characters (such as punctuations). \n",
    "* **Stop words removal**: Removing commonly used words.\n",
    "\n",
    "為了使我們的模型更容易學習，我們通常會對數據進行一些預處理步驟。這在處理文本數據時尤其重要。下面是一些文本預處理的例子：\n",
    "\n",
    "- **分詞**: 將句子符號化為單詞。\n",
    "- **字母轉小寫**: 將所有字母改成小寫。\n",
    "- **去除噪音**: 去除特殊字符（如標點符號）。\n",
    "- **去掉停頓詞**: 去除常用的單詞。\n",
    "\n",
    "Which preprocessing steps are necessary is determined by the task at hand. For example, although it is useful to remove special characters in some tasks, for others they may be important (for example, if we are dealing with multiple languages). For our task, we will lowercase our words and tokenize. \n",
    "\n",
    "哪些預處理步驟是必要的會由手上的任務決定。例如，盡管在某些任務中刪除特殊字符是有用的，但對於其他任務來說，它們可能是重要的（例如，如果我們正在處理多種語言）。對於我們的任務來說，會把單詞轉為小寫並進行標記。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTGn8ANTzZXT",
    "outputId": "aeb015b0-0b1b-4913-e87a-dec97297b53c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The preprocessing function we will use to generate our training examples\n",
    "# Our function is a simple one, we lowercase the letters\n",
    "# and then tokenize the words.\n",
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "# Create our training set\n",
    "train_sentences = [sent.lower().split() for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4jzo5tp0Hza"
   },
   "source": [
    "For each training example we have, we should also have a corresponding label. Recall that the goal of our model was to determine which words correspond to a `LOCATION`. That is, we want our model to output `0` for all the words that are not `LOCATION`s and `1` for the ones that are `LOCATION`s.\n",
    "\n",
    "對於我們擁有的每個訓練例子，應該有一個相應的標籤。回顧一下，模型的目標是確定哪些詞對應於 `LOCATION`。也就是說，我們希望模型對所有不是 `LOCATION `的詞輸出 `0`，對是 `LOCATION `的詞輸出 `1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wo1kcMAHFw7",
    "outputId": "8bddc94b-5533-4a3d-cddb-47160a150a0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgVKH9M3RtPx"
   },
   "source": [
    "#### Converting Words to Embeddings 將單詞轉換為嵌入\n",
    "\n",
    "Let's look at our training data a little more closely. Each datapoint we have is a sequence of words. On the other hand, we know that machine learning models work with numbers in vectors. How are we going to turn words into numbers? You may be thinking embeddings and you are right!\n",
    "\n",
    "讓我們更仔細地看一下我們的訓練數據。每個數據點都是一個單詞序列。另一方面，我們知道機器學習模型是用向量中的數字工作的。如何將單詞變成數字呢？你可能在想詞嵌入，你是對的！\n",
    "\n",
    "Imagine that we have an embedding lookup table `E`, where each row corresponds to an embedding. That is, each word in our vocabulary would have a corresponding embedding row `i` in this table. Whenever we want to find an embedding for a word, we will follow these steps:\n",
    "1. Find the corresponding index `i` of the word in the embedding table: `word->index`.\n",
    "2. Index into the embedding table and get the embedding: `index->embedding`.\n",
    "\n",
    "想像一下，我們有一個嵌入查詢表 `E`，其中每一行都對應著一個嵌入。也就是說，詞匯中的每個詞在這個表中都有一個對應的嵌入行 `i`。每當我們想找到一個詞的嵌入時，將遵循以下步驟：\n",
    "1. 找到該詞在嵌入表中的對應索引 `i`: `詞->索引`。\n",
    "2. 索引到嵌入表並得到嵌入: `index->embedding`。\n",
    "\n",
    "Let's look at the first step. We should assign all the words in our vocabulary to a corresponding index. We can do it as follows:\n",
    "1. Find all the unique words in our corpus.\n",
    "2. Assign an index to each.\n",
    "\n",
    "讓我們來看看第一步。我們應該將詞匯表中的所有單詞分配給一個相應的索引。可以按以下方式進行：\n",
    "1. 在我們的語料庫中找到所有獨特的詞。\n",
    "2. 為每個詞分配一個索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjTDlfPyVp5z",
    "outputId": "25aeef29-44d2-4bc6-a101-b4080e370ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the unique words in our corpus\n",
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOxnKznOWXSC"
   },
   "source": [
    "`vocabulary` now contains all the words in our corpus. On the other hand, during the test time, we can see words that are not contained in our vocabulary. If we can figure out a way to represent the unknown words, our model can still reason about whether they are a `LOCATION` or not, since we are also looking at the neighboring words for each prediction. \n",
    "\n",
    "We introduce a special token, `<unk>`, to tackle the words that are out of vocabulary. We could pick another string for our unknown token if we wanted. The only requirement here is that our token should be unique: we should only be using this token for unknown words. We will also add this special token to our vocabulary. \n",
    "\n",
    "`vocabulary` 現在包含了我們語料庫中的所有單詞。另一方面，在測試期間，我們可以看到不包含在我們的詞匯表中的詞。如果我們能想出一種方法來表示這些未知的詞，我們的模型仍然可以推理出它們是否是一個 `LOCATION`，因為我們也在看每個預測的鄰近詞。\n",
    "\n",
    "我們引入了一個特殊的標記，`<unk>`，以解決詞匯量以外的詞。如果我們願意的話，可以選擇另一個字符串作為未知標記。這裡唯一的要求是：標記應該是唯一的：我們應該只對未知的詞使用這個標記。我們還會把這個特殊的標記添加到我們的詞匯表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "ygxYuE1DYeR3"
   },
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsf4haL94AFu"
   },
   "source": [
    "Earlier we mentioned that our task was called `Word Window Classification` because our model is looking at the surroundings words in addition to the given word when it needs to make a prediction. \n",
    "\n",
    "前面我們提到，我們的任務被稱為「字窗分類」，因為我們的模型在需要進行預測時，除了給定的單詞外，還要查看周圍的單詞。\n",
    "\n",
    "For example, let's take the sentence \"We always come to Paris\". The corresponding training label for this sentence is `0, 0, 0, 0, 1` since only Paris, the last word, is a `LOCATION`. In one pass (meaning a call to `forward()`), our model will try to generate the correct label for one word. Let's say our model is trying to generate the correct label `1` for `Paris`. If we only allow our model to see `Paris`, but nothing else, we will miss out on the important information that the word `to` often times appears with `LOCATION`s.\n",
    "\n",
    "例如「我們總是來巴黎」這個句子。這個句子對應的訓練標籤是 `0, 0, 0, 0, 1`，因為只有最後一個詞巴黎是一個「地點」。在一次傳遞中（指呼叫 `forward()`），我們的模型將嘗試為一個詞生成正確的標籤。假設我們的模型正試圖為「巴黎」生成正確的標籤 `1`。如果我們只允許我們的模型看到「巴黎」，而其他東西全部忽略的話，我們會錯過 `to` 這個詞經常與 `LOCATION` 一起出現的重要資訊。\n",
    "\n",
    "Word windows allow our model to consider the surrounding `+N` or `-N` words of each word when making a prediction. In our earlier example for `Paris`, if we have a window size of 1, that means our model will look at the words that come immediately before and after `Paris`, which are `to`, and, well, nothing. Now, this raises another issue. `Paris` is at the end of our sentence, so there isn't another word following it. Remember that we define the input dimensions of our `PyTorch` models when we are initializing them. If we set the window size to be `1`, it means that our model will be accepting `3` words in every pass. We cannot have our model expect `2` words from time to time.\n",
    "\n",
    "詞窗允許我們的模型在進行預測時考慮每個詞的周圍 `+N` 或 `-N` 個詞。在我們前面關於「巴黎」的例子中，如果窗口大小為 `1`，這意味著模型將查看緊接在「巴黎」之前和之後的詞，也就是只有 `to`，其他什麼也沒有。這提出了另一個問題。\n",
    "「巴黎」在句子的末尾，所以它後面沒有別的詞。記得，我們在初始化 `PyTorch` 模型的時候定義了輸入大小。如果我們將窗口大小設置為 `1`，這意味著模型將在每一次傳遞中必須輸入 `3` 個詞。我們無法讓模型只輸入 `2` 個單詞。\n",
    "\n",
    "The solution is to introduce a special token, such as `<pad>`, that will be added to our sentences to make sure that every word has a valid window around them. Similar to `<unk>` token, we could pick another string for our pad token if we wanted, as long as we make sure it is used for a unique purpose. \n",
    "\n",
    "解決方案是引入一個特殊的標記，如 `<pad>`，它將被添加到我們的句子中，以確保每個單詞周圍都有一個有效的窗口。與 `<unk>` 標記類似，如果我們願意，我們可以選擇另一個字符串作為 padding 標記，只要確保它被用於一個獨特的目的即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVQsjYi6ZegI",
    "outputId": "7e74121d-6c0c-4763-c213-4fa3b87e845b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the <pad> token to our vocabulary\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "\n",
    "# Function that pads the given sentence\n",
    "# We are introducing this function here as an example\n",
    "# We will be utilizing it later in the tutorial\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvgWKwSNpAd"
   },
   "source": [
    "Now that our vocabularly is ready, let's assign an index to each of our words. \n",
    "\n",
    "現在詞匯表已經準備好了，讓我們為每個詞指定一個索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNCTQnKDa4oh",
    "outputId": "dca1ee13-997d-4b23-e057-3d56cfae31d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "pt-0SK67hMVo",
    "outputId": "40dfcf68-9a38-430f-993e-2b165cb8e7cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELZuteqbdWd1"
   },
   "source": [
    "Great! We are ready to convert our training sentences into a sequence of indices corresponding to each token. \n",
    "\n",
    "很好！ 我們已經準備好將訓練句子轉換成對應於每個標記的索引序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNOxip15bMfH",
    "outputId": "5b4fd72c-d287-418e-b8d4-a53ac46df4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        # Check if the token is in our vocabularly. If it is, get it's index.\n",
    "        # If not, get the index for the unknown token.\n",
    "        if token in word_to_ix:\n",
    "            index = word_to_ix[token]\n",
    "        else:\n",
    "            index = word_to_ix[\"<unk>\"]\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# More compact version of the same function\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jsXw8cB1xpH"
   },
   "source": [
    "In the example above, `kuwait` shows up as `<unk>`, because it is not included in our vocabulary. Let's convert our `train_sentences` to `example_padded_indices`. \n",
    "\n",
    "在上面的例子中，`kuwait` 顯示為 `<unk>`，因為它不包括在我們的詞匯表中。讓我們把我們的 `train_sentences` 轉換為`example_padded_indices`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRaKQwSJH-1d",
    "outputId": "fdcec0e7-d47e-41d4-e1d4-5261026b3c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZULjHBjHsEK"
   },
   "source": [
    "Now that we have an index for each word in our vocabularly, we can create an embedding table with `nn.Embedding` class in `PyTorch`. It is called as follows `nn.Embedding(num_words, embedding_dimension)` where `num_words` is the number of words in our vocabulary and the `embedding_dimension` is the dimension of the embeddings we want to have. There is nothing fancy about `nn.Embedding`: it is just a wrapper class around a trainabe `NxE` dimensional tensor, where `N` is the number of words in our vocabulary and `E` is the number of embedding dimensions. This table is initially random, but it will change over time. As we train our network, the gradients will be backpropagated all the way to the embedding layer, and hence our word embeddings would be updated. We will initiliaze the embedding layer we will use for our model in our model, but we are showing an example here. \n",
    "\n",
    "現在我們有了詞匯中每個詞的索引，我們可以用 `PyTorch` 中的 `nn.Embedding` 類創建一個嵌入表。具體呼叫方法如下： `nn.Embedding(num_words, embedding_dimension)` 其中 `num_words` 是詞匯表中的單詞數，`embedding_dimension` 是嵌入維度。 `nn.Embedding` 沒有什麼花俏的東西：它只是一個可訓練的 `NxE` 維度的張量的包裝類別，其中 `N` 是我們詞匯表中的詞數，`E` 是嵌入維度的數量。這個表最初是隨機的，但它會隨著時間的推移而改變。當我們訓練網路時，梯度將被反向傳播到嵌入層，因此詞嵌入將被更新。我們會在之後的模型中初始化嵌入層，但我們還是在這裡展示一個例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4AgHzv91VXx",
    "outputId": "82d54529-5f39-4554-a2d7-d01984bc3656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.6441, -0.7673, -1.5417, -0.1207, -0.9651],\n",
       "         [-1.2125, -0.2414, -0.7353, -0.8564, -0.6710],\n",
       "         [-0.8709, -0.2147,  0.9281, -0.2203, -1.0247],\n",
       "         [-0.7629,  0.5375, -0.2861, -0.0471,  0.1966],\n",
       "         [-0.2590, -0.6950, -0.1008,  0.2827, -1.4766],\n",
       "         [-0.4554, -2.2830, -0.9473, -1.1503, -0.8882],\n",
       "         [-0.8901, -0.4013, -0.7572, -0.1856, -0.3930],\n",
       "         [ 2.0512,  1.6718,  0.4780, -0.9675, -1.3268],\n",
       "         [-1.9501,  0.4185,  0.1948,  0.9153,  0.8057],\n",
       "         [-0.4385,  1.5488, -1.4032, -0.8632, -1.2927],\n",
       "         [ 0.1379, -0.0428,  0.9655, -0.5298,  2.1228],\n",
       "         [-0.7229,  0.0037,  0.3245,  0.0244, -1.3044],\n",
       "         [-0.9340, -0.6550,  0.4217, -0.5786, -0.3533],\n",
       "         [-0.9831,  0.0045, -0.6370, -0.5370, -0.4228],\n",
       "         [ 0.7762, -1.0065,  0.9537, -1.6010,  1.3405],\n",
       "         [-0.3111, -0.1917, -0.5288, -1.4382, -0.7878],\n",
       "         [ 0.5280, -0.2763,  1.1959, -0.4081,  0.1691],\n",
       "         [-0.2543, -1.0614, -0.5734,  0.2609, -0.7180],\n",
       "         [ 1.0990, -0.8835,  1.0079,  1.0443, -0.6036],\n",
       "         [-1.3020, -0.7434,  1.6409, -0.8391,  1.4147],\n",
       "         [ 0.2593, -0.0320, -1.4339, -0.2949, -1.4558],\n",
       "         [-0.4444, -0.9878, -0.8890,  1.2089, -0.9578],\n",
       "         [-1.0354,  1.1447,  0.2336,  1.9162,  0.4229]], requires_grad=True)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an embedding table for our words\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Printing the parameters in our embedding table\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI7ZTt4OkpPp"
   },
   "source": [
    "To get the word embedding for a word in our vocabulary, all we need to do is to create a lookup tensor. The lookup tensor is just a tensor containing the index we want to look up. `nn.Embedding` class expects an index tensor that is of type Long Tensor, so we should create our tensor accordingly. \n",
    "\n",
    "為了得到我們詞匯表中一個詞的詞嵌入，我們需要做的就是創建一個查找張量。查找張量只是一個包含我們想要查詢的索引的張量。`nn.Embedding` 類別希望索引張量是 Long Tensor 類別，所以我們應該相應地創建我們的張量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkldmcepjfh_",
    "outputId": "f759fb45-7ff0-48ed-f14a-e2770ffce8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3111, -0.1917, -0.5288, -1.4382, -0.7878],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUsdwBOxm6B4",
    "outputId": "3c437edd-93b4-4ca2-d119-b65d35f38cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3111, -0.1917, -0.5288, -1.4382, -0.7878],\n",
       "        [-0.7629,  0.5375, -0.2861, -0.0471,  0.1966]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bgSW3LPltkF"
   },
   "source": [
    "Usually, we define the embedding layer as part of our  model, which you will see in the later sections of our notebook. \n",
    "\n",
    "通常，我們將嵌入層定義為我們模型的一部分，你將在我們筆記本的後面章節中看到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHCXeQOamHU1"
   },
   "source": [
    "#### Batching Sentences 分批句子\n",
    "\n",
    "We have learned about batches in class. Waiting our whole training corpus to be processed before making an update is constly. On the other hand, updating the parameters after every training example causes the loss to be less stable between updates. To combat these issues, we instead update our parameters after training on a batch of data. This allows us to get a better estimate of the gradient of the global loss. In this section, we will learn how to structure our data into batches using the `torch.util.data.DataLoader` class. \n",
    "\n",
    "我們在課堂上已經了解了批次的問題。等待我們的整個訓練語料庫被處理後再進行更新是很有必要的。另一方面，在每個訓練實例之後更新參數會導致在兩次更新之間損失的穩定性降低。為了解決這些問題，我們在對一批數據進行訓練後再更新我們的參數。這使我們能夠更好地估計全局損失的梯度。在本節中，我們將學習如何使用 `torch.util.data.DataLoader `類將我們的數據結構化為批次。\n",
    "\n",
    "We will be calling the `DataLoader` class as follows: `DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`.  The `batch_size` parameter determines the number of examples per batch. In every epoch, we will be iterating over all the batches using the `DataLoader`. The order of batches is deterministic by default, but we can ask `DataLoader` to shuffle the batches by setting the `shuffle` parameter to `True`. This way we ensure that we don't encounter a bad batch multiple times.\n",
    "\n",
    "我們將以如下方式呼叫 `DataLoader` 類別。`DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`。 `batch_size` 參數決定了每批的例子數量。在每個 `epoch` 中，我們將使用 `DataLoader` 對所有批次進行迭代。批次的順序在默認情況下是確定的，但是我們可以通過設置 `shuffle `參數為 `True` 來要求 `DataLoader` 對批次進行清洗。這樣我們就能確保不會多次遇到一個壞的批次。\n",
    "\n",
    "If provided, `DataLoader` passes the batches it prepares to the `collate_fn`. We can write a custom function to pass to the `collate_fn` parameter in order to print stats about our batch or perform extra processing. In our case, we will use the `collate_fn` to:\n",
    "1. Window pad our train sentences.\n",
    "2. Convert the words in the training examples to indices.\n",
    "3. Pad the training examples so that all the sentences and labels have the same length. Similarly, we also need to pad the labels. This creates an issue because when calculating the loss, we need to know the actual number of words in a given example. We will also keep track of this number in the function we pass to the `collate_fn` parameter.\n",
    "\n",
    "如果提供的話，`DataLoader`會把它準備好的 `batch` 處理傳遞給 `collate_fn`。我們可以寫一個自定義函數傳遞給 `collate_fn` 參數，以便印出關於批次的統計訊息或執行額外的處理。在我們的案例中，我們將使用 `collate_fn` 來：\n",
    "1. 對我們的訓練句子進行窗口填充（pad）。\n",
    "2. 將訓練實例中的單詞轉換為索引。\n",
    "3. 對訓練例子進行填充，使所有的句子和標籤具有相同的長度。同樣地，我們也需要對標籤進行填充。這就產生了一個問題，因為在計算損失時，我們需要知道一個給定例子中的實際單詞數。我們也將在傳遞給 `collate_fn` 參數的函數中紀錄這個數字。\n",
    "\n",
    "Because our version of the `collate_fn` function will need to access to our `word_to_ix` dictionary (so that it can turn words into indices), we will make use of the `partial` function in `Python`, which passes the parameters we give to the function we pass it. \n",
    "\n",
    "因為我們的版本的 `collate_fn` 函數會需要使用 `word_to_ix` 字典（以便它能將單詞變成索引），我們將利用 `Python` 中的 `partial` 函數，它將我們給出的參數傳遞給我們傳遞給它的函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "OkvvVlo4jgFm"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    # Break our batch into the training examples (x) and labels (y)\n",
    "    # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n",
    "    # method expects tensors. This is also useful since our model will be\n",
    "    # expecting tensor inputs.\n",
    "    x, y = zip(*batch)\n",
    "\n",
    "    # Now we need to window pad our training examples. We have already defined a\n",
    "    # function to handle window padding. We are including it here again so that\n",
    "    # everything is in one place.\n",
    "    def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "        window = [pad_token] * window_size\n",
    "        return window + sentence + window\n",
    "\n",
    "    # Pad the train examples.\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "    # Now we need to turn words in our training examples to indices. We are\n",
    "    # copying the function defined earlier for the same reason as above.\n",
    "    def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "        return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "    # Convert the train examples into indices.\n",
    "    x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "    # We will now pad the examples so that the lengths of all the example in\n",
    "    # one batch are the same, making it possible to do matrix operations.\n",
    "    # We set the batch_first parameter to True so that the returned matrix has\n",
    "    # the batch as the first dimension.\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "\n",
    "    # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # We will also pad the labels. Before doing so, we will record the number\n",
    "    # of labels so that we know how many words existed in each example.\n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    # We are now ready to return our variables. The order we return our variables\n",
    "    # here will match the order we read them in our training loop.\n",
    "    return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q0gXea-bCsz"
   },
   "source": [
    "This function seems long, but it really doesn't have to be. Check out the alternative version below where we remove the extra function declarations and comments. \n",
    "\n",
    "這個函數看起來很長，但它真的不需要。請看下面的替代版本，我們刪除了多餘的函數宣告和註解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "dZfcmAJXbLcq"
   },
   "outputs": [],
   "source": [
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    # Prepare the datapoints\n",
    "    x, y = zip(*batch)\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "    x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "    # Pad x so that all the examples in the batch have the same size\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "    # Pad y and record the length\n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS1WuQO0Khxx"
   },
   "source": [
    "Now, we can see the `DataLoader` in action. \n",
    "\n",
    "現在，我們可以看到 `DataLoader` 在執行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfB0JKL2vZ6p",
    "outputId": "ce927b66-b9be-4045-fb45-87a9184e0629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([5, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0],\n",
      "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 5])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(\"Batched Input:\")\n",
    "    print(batched_x)\n",
    "    print(\"Batched Labels:\")\n",
    "    print(batched_y)\n",
    "    print(\"Batched Lengths:\")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93QOZSsMTFNF"
   },
   "source": [
    "The batched input tensors you see above will be passed into our model. On the other hand, we started off saying that our model will be a window classifier. The way our input tensors are currently formatted, we have all the words in a sentence in one datapoint. When we pass this input to our model, it needs to create the windows for each word, make a prediction as to whether the center word is a `LOCATION` or not for each window, put the predictions together and return. \n",
    "\n",
    "你在上面看到的分批輸入的張量將被傳入我們的模型。另一方面，我們一開始就說我們的模型將是一個窗口分類器。我們的輸入張量目前的格式是，我們把一個句子中的所有單詞都放在一個資料點上。當我們把這個輸入傳遞給我們的模型時，它需要為每個詞創建窗口，對每個窗口的中心詞是否是 `LOCATION` 進行預測，把預測結果放在一起並返回。\n",
    "\n",
    "We could avoid this problem if we formatted our data by breaking it into windows beforehand. In this example, we will instead how our model take care of the formatting. \n",
    "\n",
    "如果我們事先將資料分解成窗口，就可以避免這個問題。在這個例子中，我們將改用我們的模型來處理格式化的問題。\n",
    "\n",
    "Given that our `window_size` is `N` we want our model to make a prediction on every `2N+1` tokens. That is, if we have an input with `9` tokens, and a `window_size` of `2`, we want our model to return `5` predictions. This makes sense because before we padded it with `2` tokens on each side, our input also had `5` tokens in it! \n",
    "\n",
    "鑒於我們的窗口大小是 `N`，我們希望我們的模型能夠對每一個 `2N+1` 的標記進行預測。也就是說，如果我們有一個包含 `9` 個標記的輸入，而`window_size` 為 `2`，我們希望我們的模型能夠返回 `5` 個預測。這是有道理的，因為在我們用 `2` 標記填充它之前，我們的輸入也有 `5` 標記！\"。\n",
    "\n",
    "We can create these windows by using for loops, but there is a faster `PyTorch` alternative, which is the `unfold(dimension, size, step)` method. We can create the windows we need using this method as follows:\n",
    "\n",
    "我們可以通過使用 `for` 迴圈來創建這些窗口，但是有一個更快的 `PyTorch` 替代方法，那就是 `unfold(dimension, size, step)` 函式。我們可以用這個方法創建我們需要的窗口，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMZu-pxLVxHQ",
    "outputId": "8cf51a4e-5432-4d3d-9253-631648ce6a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 19,  5, 14],\n",
      "         [ 0, 19,  5, 14, 21],\n",
      "         [19,  5, 14, 21, 12],\n",
      "         [ 5, 14, 21, 12,  3],\n",
      "         [14, 21, 12,  3,  0],\n",
      "         [21, 12,  3,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# Print the original tensor\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size * 2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlDbOpeoSKxd"
   },
   "source": [
    "### Model 模型\n",
    "\n",
    "Now that we have prepared our data, we are ready to build our model. We have learned how to write custom `nn.Module` classes. We will do the same here and put everything we have learned so far together. \n",
    "\n",
    "現在我們已經準備好了資料，可以準備建立模型了。我們已經學會了如何編寫自定義的 `nn.Module` 類別。我們將在這裡做同樣的事情，把我們到目前為止所學的一切放在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "JLTU4h76NLYm"
   },
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        \"\"\" Instance variables \"\"\"\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "        \"\"\" Embedding Layer \n",
    "    Takes in a tensor containing embedding indices, and returns the \n",
    "    corresponding embeddings. The output is of dim \n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change. \n",
    "\n",
    "    \"\"\"\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "        full_window_size = 2 * window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), nn.Tanh())\n",
    "\n",
    "        \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        \"\"\" Probabilities \n",
    "    \"\"\"\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "\n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "\n",
    "        \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S) \n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avy1fnyAvEcd"
   },
   "source": [
    "### Training 訓練\n",
    "\n",
    "We are now ready to put everything together. Let's start with preparing our data and intializing our model. We can then intialize our optimizer and define our loss function. This time, instead of using one of the predefined loss function as we did before, we will define our own loss function. \n",
    "\n",
    "我們現在已經準備好把所有東西放在一起。讓我們先準備好我們的數據並初始化我們的模型。然後，我們可以初始化我們的優化器並定義我們的損失函數。這一次，我們將定義我們自己的損失函數，而不是像以前那樣使用預定義的損失函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "bInu1VqjHsfj"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the\n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHxpxDkFHfQE"
   },
   "source": [
    "Unlike our earlier example, this time instead of passing all of our training data to the model at once in each epoch, we will be utilizing batches. Hence, in each training epoch iteration, we also iterate over the batches.\n",
    "\n",
    "與我們之前的例子不同，這次我們不是在每個 `epoch` 中一次性將所有的訓練數據傳遞給模型，而是利用批次。因此，在每個 `epoch` 的迭代中，我們也會對批次進行迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "QL9IDgIOvHca"
   },
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "    # Keep track of the total loss for the batch\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run a forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        # Compute the batch loss\n",
    "        loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the parameteres\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "    # Iterate through each epoch and call our train_epoch function\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjf75cnzJ4n6"
   },
   "source": [
    "Let's start training!\n",
    "\n",
    "開始訓練吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kav8kwVBJ6XW",
    "outputId": "9b59a874-0924-4b10-b592-caafa2d8ff0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2729293256998062\n",
      "0.22947931662201881\n",
      "0.18051733449101448\n",
      "0.13749945163726807\n",
      "0.08919842541217804\n",
      "0.07945763878524303\n",
      "0.05936894100159407\n",
      "0.05264806840568781\n",
      "0.0382453016936779\n",
      "0.035388316959142685\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-k7Pav4LdQJ"
   },
   "source": [
    "### Prediction 預測\n",
    "\n",
    "Let's see how well our model is at making predictions. We can start by creating our test data.\n",
    "\n",
    "讓我們看看我們的模型在預測方面的表現如何。我們可以從創建我們的測試數據開始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "-v5X69a2Lkbm"
   },
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlBa8xaNMZgv"
   },
   "source": [
    "Let's loop over our test examples to see how well we are \n",
    "doing. \n",
    "\n",
    "讓我們在測試實例上預測，看看我們做得如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGYn8CAoMTjX",
    "outputId": "e6cfd4fb-fbb6-4818-fcdf-92dacbffe09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.5522, 0.0443, 0.0483, 0.9403]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(labels)\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}